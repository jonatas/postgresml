{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":".md-typeset h1 { font-size: 3em; font-weight: 700; margin-bottom: -1rem; max-width: 80em; } .md-typeset p.subtitle { font-weight: 100; margin: 2em; max-width: 80em; } .md-typeset img { margin: 0; border-radius: 10px; } .md-grid { max-width: 100em; } .md-content video, .md-content img { max-width: 90%; margin: 2em 5%; } article.md-content__inner.md-typeset a.md-content__button.md-icon { display: none; } End-to-end machine learning solution for everyone Train and deploy models to make online predictions using only SQL, with an open source extension for Postgres. Manage your projects and visualize datasets using the built-in dashboard. Demo Pure SQL Solution \u00b6 train.sql 1 2 3 4 5 6 7 SELECT pgml . train ( 'My project name' , task => 'regression' , relation_name => 'my_table_with_data' , y_column_name => 'my_column_with_labels' , algorithm => 'xgboost' ); Learn more about Training deploy.sql 1 2 3 4 5 SELECT pgml . deploy ( 'My project name' , strategy => 'most_recent' , algorithm => 'xgboost' ); Learn more about Deployments predict.sql 1 2 3 4 5 6 SELECT * , pgml . predict ( 'My project name' , ARRAY [...] -- same features used in training ) AS prediction FROM my_new_unlabeled_table ORDER BY prediction DESC ; Learn more about Predictions Get Started What's in the box \u00b6 All your favorite algorithms Whether you need a simple linear regression, or extreme gradient boosting, we've included support for all classification and regression algorithms in Scikit Learn and XGBoost with no extra configuration. Algorithms Instant visualizations Run standard analysis on your datasets to detect outliers, bimodal distributions, feature correlation, and other common data visualizations on your datasets. Everything is cataloged in the dashboard for easy reference. Dashboard Hyperparameter search Use either grid or random searches with cross validation on your training set to discover the most important knobs to tweak on your favorite algorithm. Hyperparameter Search Online and offline support Predictions are served via a standard Postgres connection to ensure that your core apps can always access both your data and your models in real time. Pure SQL workflows also enable batch predictions to cache results in native Postgres tables for lookup. Predictions SQL native vector operations Vector operations make working with learned emebeddings a snap, for things like nearest neighbor searches or other similarity comparisons. Vector Operations Managed model deployments Models can be periodically retrained and automatically promoted to production depending on their key metric. Rollback capability is provided to ensure that you're always able to serve the highest quality predictions, along with historical logs of all deployments for long term study. Deployments The performance of Postgres Since your data never leaves the database, you retain the speed, reliability and security you expect in your foundational stateful services. Leverage your existing infrastructure and the data distribution strategies native to PostgreSQL to deliver new capabilities. Distributed Training Open source We're building on the shoulders of giants. These machine learning libraries and Postgres have recieved extensive academic and industry use, and we'll continue their tradition to build with the community. MIT License","title":"Home"},{"location":"#pure-sql-solution","text":"train.sql 1 2 3 4 5 6 7 SELECT pgml . train ( 'My project name' , task => 'regression' , relation_name => 'my_table_with_data' , y_column_name => 'my_column_with_labels' , algorithm => 'xgboost' ); Learn more about Training deploy.sql 1 2 3 4 5 SELECT pgml . deploy ( 'My project name' , strategy => 'most_recent' , algorithm => 'xgboost' ); Learn more about Deployments predict.sql 1 2 3 4 5 6 SELECT * , pgml . predict ( 'My project name' , ARRAY [...] -- same features used in training ) AS prediction FROM my_new_unlabeled_table ORDER BY prediction DESC ; Learn more about Predictions Get Started","title":"Pure SQL Solution"},{"location":"#whats-in-the-box","text":"All your favorite algorithms Whether you need a simple linear regression, or extreme gradient boosting, we've included support for all classification and regression algorithms in Scikit Learn and XGBoost with no extra configuration. Algorithms Instant visualizations Run standard analysis on your datasets to detect outliers, bimodal distributions, feature correlation, and other common data visualizations on your datasets. Everything is cataloged in the dashboard for easy reference. Dashboard Hyperparameter search Use either grid or random searches with cross validation on your training set to discover the most important knobs to tweak on your favorite algorithm. Hyperparameter Search Online and offline support Predictions are served via a standard Postgres connection to ensure that your core apps can always access both your data and your models in real time. Pure SQL workflows also enable batch predictions to cache results in native Postgres tables for lookup. Predictions SQL native vector operations Vector operations make working with learned emebeddings a snap, for things like nearest neighbor searches or other similarity comparisons. Vector Operations Managed model deployments Models can be periodically retrained and automatically promoted to production depending on their key metric. Rollback capability is provided to ensure that you're always able to serve the highest quality predictions, along with historical logs of all deployments for long term study. Deployments The performance of Postgres Since your data never leaves the database, you retain the speed, reliability and security you expect in your foundational stateful services. Leverage your existing infrastructure and the data distribution strategies native to PostgreSQL to deliver new capabilities. Distributed Training Open source We're building on the shoulders of giants. These machine learning libraries and Postgres have recieved extensive academic and industry use, and we'll continue their tradition to build with the community. MIT License","title":"What's in the box"},{"location":"about/faq/","text":"FAQ \u00b6 How far can this scale? Petabyte-sized Postgres deployments are documented in production since at least 2008, and recent patches have enabled working beyond exabyte and up to the yotabyte scale. Machine learning models can be horizontally scaled using standard Postgres replicas. How reliable can this be? Postgres is widely considered mission critical, and some of the most reliable technology in any modern stack. PostgresML allows an infrastructure organization to leverage pre-existing best practices to deploy machine learning into production with less risk and effort than other systems. For example, model backup and recovery happens automatically alongside normal Postgres data backup. How good are the models? Model quality is often a tradeoff between compute resources and incremental quality improvements. Sometimes a few thousands training examples and an off the shelf algorithm can deliver significant business value after a few seconds of training. PostgresML allows stakeholders to choose several different algorithms to get the most bang for the buck, or invest in more computationally intensive techniques as necessary. In addition, PostgresML automatically applies best practices for data cleaning like imputing missing values by default and normalizing data to prevent common problems in production. PostgresML doesn't help with reformulating a business problem into a machine learning problem. Like most things in life, the ultimate in quality will be a concerted effort of experts working over time. PostgresML is intended to establish successful patterns for those experts to collaborate around while leveraging the expertise of open source and research communities. Is PostgresML fast? Colocating the compute with the data inside the database removes one of the most common latency bottlenecks in the ML stack, which is the (de)serialization of data between stores and services across the wire. Modern versions of Postgres also support automatic query parrellization across multiple workers to further minimize latency in large batch workloads. Finally, PostgresML will utilize GPU compute if both the algorithm and hardware support it, although it is currently rare in practice for production databases to have GPUs. We're working on benchmarks .","title":"FAQ"},{"location":"about/faq/#faq","text":"How far can this scale? Petabyte-sized Postgres deployments are documented in production since at least 2008, and recent patches have enabled working beyond exabyte and up to the yotabyte scale. Machine learning models can be horizontally scaled using standard Postgres replicas. How reliable can this be? Postgres is widely considered mission critical, and some of the most reliable technology in any modern stack. PostgresML allows an infrastructure organization to leverage pre-existing best practices to deploy machine learning into production with less risk and effort than other systems. For example, model backup and recovery happens automatically alongside normal Postgres data backup. How good are the models? Model quality is often a tradeoff between compute resources and incremental quality improvements. Sometimes a few thousands training examples and an off the shelf algorithm can deliver significant business value after a few seconds of training. PostgresML allows stakeholders to choose several different algorithms to get the most bang for the buck, or invest in more computationally intensive techniques as necessary. In addition, PostgresML automatically applies best practices for data cleaning like imputing missing values by default and normalizing data to prevent common problems in production. PostgresML doesn't help with reformulating a business problem into a machine learning problem. Like most things in life, the ultimate in quality will be a concerted effort of experts working over time. PostgresML is intended to establish successful patterns for those experts to collaborate around while leveraging the expertise of open source and research communities. Is PostgresML fast? Colocating the compute with the data inside the database removes one of the most common latency bottlenecks in the ML stack, which is the (de)serialization of data between stores and services across the wire. Modern versions of Postgres also support automatic query parrellization across multiple workers to further minimize latency in large batch workloads. Finally, PostgresML will utilize GPU compute if both the algorithm and hardware support it, although it is currently rare in practice for production databases to have GPUs. We're working on benchmarks .","title":"FAQ"},{"location":"about/license/","text":"Copyright \u00a9 2022 PostgresML Team Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"License"},{"location":"about/motivation/","text":"Motivation \u00b6 Deploying machine learning models into existing applications is not straight forward. It involves operating new services, which need to be written in specialized languages with libraries outside of the experience of many software engineers. Those services tend to be architected around specialized datastores and hardware that requires additional management and know how. Data access needs to be secure across production and development environments without impeding productivity. This complexity pushes risks and costs beyond acceptable trade off limits for many otherwise valuable use cases. PostgresML makes ML simple by moving the code to your data, rather than copying the data all over the place. You train models using simple SQL commands, and you get the predictions in your apps via a mechanism you're already using: a query over a standard Postgres connection. Our goal is that anyone with a basic understanding of SQL should be able to build, deploy and maintain machine learning models in production, while receiving the benefits of a high performance machine learning platform. Ultimately, PostgresML aims to be the easiest, safest and fastest way to gain value from machine learning.","title":"Motivation"},{"location":"about/motivation/#motivation","text":"Deploying machine learning models into existing applications is not straight forward. It involves operating new services, which need to be written in specialized languages with libraries outside of the experience of many software engineers. Those services tend to be architected around specialized datastores and hardware that requires additional management and know how. Data access needs to be secure across production and development environments without impeding productivity. This complexity pushes risks and costs beyond acceptable trade off limits for many otherwise valuable use cases. PostgresML makes ML simple by moving the code to your data, rather than copying the data all over the place. You train models using simple SQL commands, and you get the predictions in your apps via a mechanism you're already using: a query over a standard Postgres connection. Our goal is that anyone with a basic understanding of SQL should be able to build, deploy and maintain machine learning models in production, while receiving the benefits of a high performance machine learning platform. Ultimately, PostgresML aims to be the easiest, safest and fastest way to gain value from machine learning.","title":"Motivation"},{"location":"about/roadmap/","text":"Roadmap \u00b6 This project is currently a proof of concept. Some important features, which we are currently thinking about or working on, are listed below. Production deployment \u00b6 Many companies that use PostgreSQL in production do so using managed services like AWS RDS, Digital Ocean, Azure, etc. Those services do not allow running custom extensions, so we have to run PostgresML directly on VMs, e.g. EC2, droplets, etc. The idea here is to replicate production data directly from Postgres and make it available in real-time to PostgresML. We're considering solutions like logical replication for small to mid-size databases, and Debezium for multi-TB deployments. Model management dashboard \u00b6 A good looking and useful UI goes a long way. A dashboard similar to existing solutions like MLFlow or AWS SageMaker will make the experience of working with PostgresML as pleasant as possible. Data explorer \u00b6 A data explorer allows anyone to browse the dataset in production and to find useful tables and features to build effective machine learning models. More algorithms \u00b6 Scikit-Learn is a good start, but we're also thinking about including Tensorflow, Pytorch, and many more useful models. Scheduled training \u00b6 In applications where data changes often, it's useful to retrain the models automatically on a schedule, e.g. every day, every week, etc.","title":"Roadmap"},{"location":"about/roadmap/#roadmap","text":"This project is currently a proof of concept. Some important features, which we are currently thinking about or working on, are listed below.","title":"Roadmap"},{"location":"about/roadmap/#production-deployment","text":"Many companies that use PostgreSQL in production do so using managed services like AWS RDS, Digital Ocean, Azure, etc. Those services do not allow running custom extensions, so we have to run PostgresML directly on VMs, e.g. EC2, droplets, etc. The idea here is to replicate production data directly from Postgres and make it available in real-time to PostgresML. We're considering solutions like logical replication for small to mid-size databases, and Debezium for multi-TB deployments.","title":"Production deployment"},{"location":"about/roadmap/#model-management-dashboard","text":"A good looking and useful UI goes a long way. A dashboard similar to existing solutions like MLFlow or AWS SageMaker will make the experience of working with PostgresML as pleasant as possible.","title":"Model management dashboard"},{"location":"about/roadmap/#data-explorer","text":"A data explorer allows anyone to browse the dataset in production and to find useful tables and features to build effective machine learning models.","title":"Data explorer"},{"location":"about/roadmap/#more-algorithms","text":"Scikit-Learn is a good start, but we're also thinking about including Tensorflow, Pytorch, and many more useful models.","title":"More algorithms"},{"location":"about/roadmap/#scheduled-training","text":"In applications where data changes often, it's useful to retrain the models automatically on a schedule, e.g. every day, every week, etc.","title":"Scheduled training"},{"location":"about/team/","text":"ul.team img { border-radius: 50%; } Team \u00b6 Montana Low montanalow Lev Kokotov levkk","title":"Team"},{"location":"about/team/#team","text":"","title":"Team"},{"location":"developer_guide/overview/","text":"Contributing \u00b6 General \u00b6 Use unix line endings . Setup your development environment \u00b6 Install pyenv for your system to use the correct version of python specified in .python-version . Make sure your $PATH now includes ~/.pyenv/bin && ~/.pyenv/shims . Install pyenv-virtualenv to isolate project dependencies which keeps requirements.txt clean and frozen. Install the version of python listed in .python-version : $ pyenv install 3.10.4 Create the virtual env: $ pyenv virtualenv 3.10.4 pgml-dashboard Install the dependencies: $ pip install -r requirements.txt If you ever add new dependencies, freeze them: $ pip freeze > requirements.txt Make sure requirements.txt has no changes, which indicates your virtual environment is setup correctly. $ git status Setup your .env : $ cp .env.TEMPLATE .env $ nano .env Run the server: $ ./manage.py runserver Maintain your development database \u00b6 How to reset your local database: $ psql -c \"DROP DATABASE pgml_development\" postgres://postgres@127.0.0.1:5433 $ psql -c \"CREATE DATABASE pgml_development\" postgres://postgres@127.0.0.1:5433 $ psql -c \"CREATE SCHEMA pgml\" postgres://postgres@127.0.0.1:5433/pgml_development Follow the installation instructions to create a local working Postgres environment, then install the pgml-extension from the git repository: cd pgml-extension sudo python3 setup.py install sudo python3 -m pgml_extension --database-url=postgres://postgres@localhost:5433/pgml_development Run the tests from the root of the repo: cd pgml-extension ON_ERROR_STOP=1 psql -f tests/test.sql postgres://postgres@127.0.0.1:5433/pgml_development One liner: cd pgml-extension; sudo /bin/pip3 install .; psql -c \"DROP DATABASE pgml_development\" postgres://postgres@127.0.0.1:5433/; psql -c \"CREATE DATABASE pgml_development\" postgres://postgres@127.0.0.1:5433/; psql -c \"create schema pgml\" postgres://postgres@127.0.0.1:5433/pgml_development; sudo python3 -m pgml_extension --database-url=postgres://postgres@localhost:5433/pgml_development; ON_ERROR_STOP=1 psql -f tests/test.sql -P pager postgres://postgres@127.0.0.1:5433/pgml_development; cd .. Make sure to run it exactly like this, from the root directory of the repo. Update documentation \u00b6 mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Developer Overview"},{"location":"developer_guide/overview/#contributing","text":"","title":"Contributing"},{"location":"developer_guide/overview/#general","text":"Use unix line endings .","title":"General"},{"location":"developer_guide/overview/#setup-your-development-environment","text":"Install pyenv for your system to use the correct version of python specified in .python-version . Make sure your $PATH now includes ~/.pyenv/bin && ~/.pyenv/shims . Install pyenv-virtualenv to isolate project dependencies which keeps requirements.txt clean and frozen. Install the version of python listed in .python-version : $ pyenv install 3.10.4 Create the virtual env: $ pyenv virtualenv 3.10.4 pgml-dashboard Install the dependencies: $ pip install -r requirements.txt If you ever add new dependencies, freeze them: $ pip freeze > requirements.txt Make sure requirements.txt has no changes, which indicates your virtual environment is setup correctly. $ git status Setup your .env : $ cp .env.TEMPLATE .env $ nano .env Run the server: $ ./manage.py runserver","title":"Setup your development environment"},{"location":"developer_guide/overview/#maintain-your-development-database","text":"How to reset your local database: $ psql -c \"DROP DATABASE pgml_development\" postgres://postgres@127.0.0.1:5433 $ psql -c \"CREATE DATABASE pgml_development\" postgres://postgres@127.0.0.1:5433 $ psql -c \"CREATE SCHEMA pgml\" postgres://postgres@127.0.0.1:5433/pgml_development Follow the installation instructions to create a local working Postgres environment, then install the pgml-extension from the git repository: cd pgml-extension sudo python3 setup.py install sudo python3 -m pgml_extension --database-url=postgres://postgres@localhost:5433/pgml_development Run the tests from the root of the repo: cd pgml-extension ON_ERROR_STOP=1 psql -f tests/test.sql postgres://postgres@127.0.0.1:5433/pgml_development One liner: cd pgml-extension; sudo /bin/pip3 install .; psql -c \"DROP DATABASE pgml_development\" postgres://postgres@127.0.0.1:5433/; psql -c \"CREATE DATABASE pgml_development\" postgres://postgres@127.0.0.1:5433/; psql -c \"create schema pgml\" postgres://postgres@127.0.0.1:5433/pgml_development; sudo python3 -m pgml_extension --database-url=postgres://postgres@localhost:5433/pgml_development; ON_ERROR_STOP=1 psql -f tests/test.sql -P pager postgres://postgres@127.0.0.1:5433/pgml_development; cd .. Make sure to run it exactly like this, from the root directory of the repo.","title":"Maintain your development database"},{"location":"developer_guide/overview/#update-documentation","text":"mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Update documentation"},{"location":"gym/introduction/","text":"img.float-right { margin: 0 16px !important; max-width: 50% !important; } img.center { margin: 16px 12.5%; max-width: 75%; } Announcing the PostgresML Gym! \ud83c\udf89 \u00b6 A common problem with data science and machine learning tutorials is the published example data is usually nothing like what you\u2019ll find in industry. It\u2019s usually denormalized into a single tabular form, e.g. csv file It\u2019s relatively tiny to medium amounts of data, not big data It\u2019s static, new rows are never added It\u2019s often been pre-treated to clean or simplify the data As Data Science transitions from academia into industry, their norms influence organizations, applications and deployments. Professional Data Scientists need teams of Data Engineers to move the data from production databases into centralized data warehouses and denormalized schemas that they are more familiar with. Large offline batch jobs are a typical integration point between Data Scientists and their Engineering counterparts who deal with online systems. As the systems grow more complex, additional specialized Machine Learning Engineers are required to optimize performance and scalability bottlenecks between databases, warehouses, models and applications. This eventually leads to expensive maintenance, and then to terminal complexity where new improvements to the system become exponentially more difficult. Ultimately, previously working models start getting replaced by simpler solutions, so the business can continue to iterate. This is not a new phenomenon, see the fate of the Netflix Prize. Instead of starting from the academic perspective that data is dead, PostgresML embraces the living and dynamic nature of data inside modern organizations. It's relational. Schemas are normalized for OLTP use cases and real time performance considerations New rows are constantly added and updated, which form incomplete features for a prediction Denormalized datasets may grow to billions of rows, and terabytes of data The data often spans multiple versions of the schema, and bugs can introduce outliers Modern applications are interactive and real time in nature, rather than static and precomputable These are the types of considerations that might make some Data Scientists and Statisticians sigh, and maybe even twitch a little bit, but they are the bread and butter of Software Engineering. It\u2019s hard to teach Machine Learning with these considerations, because these types of datasets are non trivial or even illegal to distribute outside of their typically proprietary applications. We think it\u2019s worth attempting to move the learning process in industry beyond the status quo. To that end, we\u2019re building the PostgresML Gym to generate realistic application data in a Postgres database. For now, the gym starts as an empty Postgres database after you sign up, but you can start loading those familiar academic data sets with calls to pgml.load_data() in your own Dashboard . We'll be publishing a series of blog posts detailing common machine learning applications, that demonstrate the differences in OLTP ML vs conventional data warehouse centric approaches, and the advantages that PostgresML can provide for industrial applications. We\u2019d also love to hear feedback from the broader ML and Engineering community about applications and other real world scenarios to help prioritize our work. Sign up for the Gym","title":"Introduction"},{"location":"gym/introduction/#announcing-the-postgresml-gym","text":"A common problem with data science and machine learning tutorials is the published example data is usually nothing like what you\u2019ll find in industry. It\u2019s usually denormalized into a single tabular form, e.g. csv file It\u2019s relatively tiny to medium amounts of data, not big data It\u2019s static, new rows are never added It\u2019s often been pre-treated to clean or simplify the data As Data Science transitions from academia into industry, their norms influence organizations, applications and deployments. Professional Data Scientists need teams of Data Engineers to move the data from production databases into centralized data warehouses and denormalized schemas that they are more familiar with. Large offline batch jobs are a typical integration point between Data Scientists and their Engineering counterparts who deal with online systems. As the systems grow more complex, additional specialized Machine Learning Engineers are required to optimize performance and scalability bottlenecks between databases, warehouses, models and applications. This eventually leads to expensive maintenance, and then to terminal complexity where new improvements to the system become exponentially more difficult. Ultimately, previously working models start getting replaced by simpler solutions, so the business can continue to iterate. This is not a new phenomenon, see the fate of the Netflix Prize. Instead of starting from the academic perspective that data is dead, PostgresML embraces the living and dynamic nature of data inside modern organizations. It's relational. Schemas are normalized for OLTP use cases and real time performance considerations New rows are constantly added and updated, which form incomplete features for a prediction Denormalized datasets may grow to billions of rows, and terabytes of data The data often spans multiple versions of the schema, and bugs can introduce outliers Modern applications are interactive and real time in nature, rather than static and precomputable These are the types of considerations that might make some Data Scientists and Statisticians sigh, and maybe even twitch a little bit, but they are the bread and butter of Software Engineering. It\u2019s hard to teach Machine Learning with these considerations, because these types of datasets are non trivial or even illegal to distribute outside of their typically proprietary applications. We think it\u2019s worth attempting to move the learning process in industry beyond the status quo. To that end, we\u2019re building the PostgresML Gym to generate realistic application data in a Postgres database. For now, the gym starts as an empty Postgres database after you sign up, but you can start loading those familiar academic data sets with calls to pgml.load_data() in your own Dashboard . We'll be publishing a series of blog posts detailing common machine learning applications, that demonstrate the differences in OLTP ML vs conventional data warehouse centric approaches, and the advantages that PostgresML can provide for industrial applications. We\u2019d also love to hear feedback from the broader ML and Engineering community about applications and other real world scenarios to help prioritize our work. Sign up for the Gym","title":"Announcing the PostgresML Gym! \ud83c\udf89"},{"location":"gym/quick_start/","text":"Quick Start \u00b6 PostgresML is easy to get started with. If you haven't already, sign up for our Gym to get a free hosted PostgresML instance you can use to follow this tutorial. You can also run one yourself by following the instructions in our Github repo. Sign Up for the Gym Once you have your PostgresML instance running, we'll be ready to get started. Get data \u00b6 The fisrt part of machine learning is getting your data in a format you can use. That's usually the hardest part, but thankfully we have a few example datasets we can use. To load one of them, navigate to the IDE tab and run this query: SELECT * FROM pgml . load_dataset ( 'diabetes' ); You should see something like this: We have more example Scikit datasets available: iris (classification), digits (classification), wine (regression), To load them into PostgresML, use the same function above with the desired dataset name as parameter. They will become available in the pgml schema as pgml.iris , pgml.digits and pgml.wine respectively. Browse data \u00b6 The SQL editor you just used can run arbitrary queries on the PostgresML instance. For example, if we want to see what dataset we just loaded looks like, we can run: SELECT * FROM pgml . diabetes LIMIT 5 ; The diabetes dataset is a toy (small, not realistic) dataset published by Scikit Learn. It contains ten feature columns and one label column: Column Description Data type age The age of the patient (in years). float sex The sex of the patient (normalized). float bmi The BMI Body Mass index. float bp Average blood pressure of the patient. float s1 Total serum cholesterol. float s2 Low-density lipoproteins. float s3 High-density lipoproteins. float s4 Total cholesterol / HDL. float s5 Possibly log of serum triglycerides level. float s6 Blood sugar level. float target Quantitative measure of disease progression one year after baseline. float This dataset is not realistic because all data is perfectly arranged and normalized, which won't be the case with most real world datasets you'll run into, but it's perfect for our quick tutorial. Alright, we're ready to do some machine learning! First project \u00b6 PostgresML organizes itself into projects. A project is just a name for model(s) trained on a particular dataset. Let's create our first project by training an XGBoost regression model on our diabetes dataset. Using the IDE, run: SELECT * FROM pgml . train ( 'My First Project' , task => 'regression' , relation_name => 'pgml.diabetes' , y_column_name => 'target' , algorithm => 'xgboost' ); You should see this: By executing pmgl.train() we did the following: created a project called \"My First Project\", snapshotted the table pgml.diabetes thus making the experiment reproducible (in case data changes, as it happens in the real world), trained an XGBoost regression model on the data contained in the pgml.diabetes table using the column target as the label, deployed the model into production. We're ready to predict novel data points! Inference \u00b6 Inference is the act of predicting labels that we haven't necessarily used in training. That's the whole point of machine learning really: predict something we haven't seen before. Let's try and predict some new values. Using the IDE, run: SELECT pgml . predict ( 'My First Project' , ARRAY [ 0 . 06 , -- age 0 . 05 , -- sex 0 . 05 , -- bmi - 0 . 0056 , -- bp 0 . 012191 , -- s1 - 0 . 043401 , -- s2 0 . 034309 , -- s3 - 0 . 031938 , -- s4 - 0 . 061988 , --s5 - 0 . 031988 -- s6 ] ) AS prediction ; You should see something like this: The prediction column represents the possible value of the target column given the new features we just passed into the pgml.predict() function. You can just as easily predict multiple points and compare them to the actual labels in the dataset: SELECT pgml . predict ( 'My First Project 2' , ARRAY [ age , sex , bmi , bp , s1 , s3 , s3 , s4 , s5 , s6 ]), target FROM pgml . diabetes LIMIT 10 ; Sometimes the model will be pretty close, but sometimes it will be way off. That's why we'll be training several of them and comparing them next. Browse around \u00b6 By creating our first project, we made the Dashboard a little bit more interesting. This is how the pgml.diabetes snapshot we just created looks like: As you can see, we automatically performed some analysis on the data. Visualizing the data is important to understand how it could potentially behave given different models, and maybe even predict how it could evolve in the future. XGBoost is a good algorithm, but what if there are better ones? Let's try training a few more using the IDE. Run these one at a time: -- Simple linear regression. SELECT * FROM pgml . train ( 'My First Project' , algorithm => 'linear' ); -- The Lasso (much fancier linear regression). SELECT * FROM pgml . train ( 'My First Project' , algorithm => 'lasso' ); If you navigate to the Models tab, you should see all three algorithms you just trained: Huh, apparently XGBoost isn't as good we originally thought! In this case, a simple linear regression did significantly better than all the others. It's hard to know which algorithm will perform best given a dataset; even experienced machine learning engineers get this one wrong. With PostgresML, you needn't worry: you can train all of them and see which one does best for your data. PostgresML will automatically use the best one for inference. Conclusion \u00b6 Congratulations on becoming a Machine Learning engineer. If you thought ML was scary or mysterious, we hope that this small tutorial made it a little bit more approachable. This is the first of many tutorials we'll publish, so stay tuned. Happy machine learning!","title":"Quick Start"},{"location":"gym/quick_start/#quick-start","text":"PostgresML is easy to get started with. If you haven't already, sign up for our Gym to get a free hosted PostgresML instance you can use to follow this tutorial. You can also run one yourself by following the instructions in our Github repo. Sign Up for the Gym Once you have your PostgresML instance running, we'll be ready to get started.","title":"Quick Start"},{"location":"gym/quick_start/#get-data","text":"The fisrt part of machine learning is getting your data in a format you can use. That's usually the hardest part, but thankfully we have a few example datasets we can use. To load one of them, navigate to the IDE tab and run this query: SELECT * FROM pgml . load_dataset ( 'diabetes' ); You should see something like this: We have more example Scikit datasets available: iris (classification), digits (classification), wine (regression), To load them into PostgresML, use the same function above with the desired dataset name as parameter. They will become available in the pgml schema as pgml.iris , pgml.digits and pgml.wine respectively.","title":"Get data"},{"location":"gym/quick_start/#browse-data","text":"The SQL editor you just used can run arbitrary queries on the PostgresML instance. For example, if we want to see what dataset we just loaded looks like, we can run: SELECT * FROM pgml . diabetes LIMIT 5 ; The diabetes dataset is a toy (small, not realistic) dataset published by Scikit Learn. It contains ten feature columns and one label column: Column Description Data type age The age of the patient (in years). float sex The sex of the patient (normalized). float bmi The BMI Body Mass index. float bp Average blood pressure of the patient. float s1 Total serum cholesterol. float s2 Low-density lipoproteins. float s3 High-density lipoproteins. float s4 Total cholesterol / HDL. float s5 Possibly log of serum triglycerides level. float s6 Blood sugar level. float target Quantitative measure of disease progression one year after baseline. float This dataset is not realistic because all data is perfectly arranged and normalized, which won't be the case with most real world datasets you'll run into, but it's perfect for our quick tutorial. Alright, we're ready to do some machine learning!","title":"Browse data"},{"location":"gym/quick_start/#first-project","text":"PostgresML organizes itself into projects. A project is just a name for model(s) trained on a particular dataset. Let's create our first project by training an XGBoost regression model on our diabetes dataset. Using the IDE, run: SELECT * FROM pgml . train ( 'My First Project' , task => 'regression' , relation_name => 'pgml.diabetes' , y_column_name => 'target' , algorithm => 'xgboost' ); You should see this: By executing pmgl.train() we did the following: created a project called \"My First Project\", snapshotted the table pgml.diabetes thus making the experiment reproducible (in case data changes, as it happens in the real world), trained an XGBoost regression model on the data contained in the pgml.diabetes table using the column target as the label, deployed the model into production. We're ready to predict novel data points!","title":"First project"},{"location":"gym/quick_start/#inference","text":"Inference is the act of predicting labels that we haven't necessarily used in training. That's the whole point of machine learning really: predict something we haven't seen before. Let's try and predict some new values. Using the IDE, run: SELECT pgml . predict ( 'My First Project' , ARRAY [ 0 . 06 , -- age 0 . 05 , -- sex 0 . 05 , -- bmi - 0 . 0056 , -- bp 0 . 012191 , -- s1 - 0 . 043401 , -- s2 0 . 034309 , -- s3 - 0 . 031938 , -- s4 - 0 . 061988 , --s5 - 0 . 031988 -- s6 ] ) AS prediction ; You should see something like this: The prediction column represents the possible value of the target column given the new features we just passed into the pgml.predict() function. You can just as easily predict multiple points and compare them to the actual labels in the dataset: SELECT pgml . predict ( 'My First Project 2' , ARRAY [ age , sex , bmi , bp , s1 , s3 , s3 , s4 , s5 , s6 ]), target FROM pgml . diabetes LIMIT 10 ; Sometimes the model will be pretty close, but sometimes it will be way off. That's why we'll be training several of them and comparing them next.","title":"Inference"},{"location":"gym/quick_start/#browse-around","text":"By creating our first project, we made the Dashboard a little bit more interesting. This is how the pgml.diabetes snapshot we just created looks like: As you can see, we automatically performed some analysis on the data. Visualizing the data is important to understand how it could potentially behave given different models, and maybe even predict how it could evolve in the future. XGBoost is a good algorithm, but what if there are better ones? Let's try training a few more using the IDE. Run these one at a time: -- Simple linear regression. SELECT * FROM pgml . train ( 'My First Project' , algorithm => 'linear' ); -- The Lasso (much fancier linear regression). SELECT * FROM pgml . train ( 'My First Project' , algorithm => 'lasso' ); If you navigate to the Models tab, you should see all three algorithms you just trained: Huh, apparently XGBoost isn't as good we originally thought! In this case, a simple linear regression did significantly better than all the others. It's hard to know which algorithm will perform best given a dataset; even experienced machine learning engineers get this one wrong. With PostgresML, you needn't worry: you can train all of them and see which one does best for your data. PostgresML will automatically use the best one for inference.","title":"Browse around"},{"location":"gym/quick_start/#conclusion","text":"Congratulations on becoming a Machine Learning engineer. If you thought ML was scary or mysterious, we hope that this small tutorial made it a little bit more approachable. This is the first of many tutorials we'll publish, so stay tuned. Happy machine learning!","title":"Conclusion"},{"location":"user_guides/dashboard/overview/","text":"Dashboard \u00b6 PostgresML comes with an app to provide visibility into models and datasets in your database. If you're running the standard docker container, you can view it running on http://localhost:8000/ . Since your pgml schema starts empty, there isn't much to see. If you'd like to generate some examples, you can run the test suite against your database. Generate example data \u00b6 The test suite for PostgresML is composed by running the sql files in the examples directory . You can use these examples to populate your local installation with some seed data. The test suite only operates on the pgml schema, and is otherwise isolated from the rest of the Postgres cluster. $ psql -f pgml-extension/sql/test.sql -P pager postgres://postgres@127.0.0.1:5433/pgml_development Overview \u00b6 Now there should be something to see in your local dashboard. Projects \u00b6 Projects organize Models that are all striving toward the same task. They aren't much more than a name to group a collection of models. You can see the currently deployed model for each project indicated by . Models \u00b6 Models are the result of training an algorithm on a Snapshot of a dataset. They record metrics depending on their projects task, and are scored accordingly. Some models are the result of a hyperparameter search, and include additional analysis on the range of hyperparameters they are tested against. Snapshots \u00b6 A Snapshot is created during training runs to record the data used for further analysis, or to train additional models against identical data. Deployments \u00b6 Every deployment is recorded to track models over time.","title":"Dashboard"},{"location":"user_guides/dashboard/overview/#dashboard","text":"PostgresML comes with an app to provide visibility into models and datasets in your database. If you're running the standard docker container, you can view it running on http://localhost:8000/ . Since your pgml schema starts empty, there isn't much to see. If you'd like to generate some examples, you can run the test suite against your database.","title":"Dashboard"},{"location":"user_guides/dashboard/overview/#generate-example-data","text":"The test suite for PostgresML is composed by running the sql files in the examples directory . You can use these examples to populate your local installation with some seed data. The test suite only operates on the pgml schema, and is otherwise isolated from the rest of the Postgres cluster. $ psql -f pgml-extension/sql/test.sql -P pager postgres://postgres@127.0.0.1:5433/pgml_development","title":"Generate example data"},{"location":"user_guides/dashboard/overview/#overview","text":"Now there should be something to see in your local dashboard.","title":"Overview"},{"location":"user_guides/dashboard/overview/#projects","text":"Projects organize Models that are all striving toward the same task. They aren't much more than a name to group a collection of models. You can see the currently deployed model for each project indicated by .","title":"Projects"},{"location":"user_guides/dashboard/overview/#models","text":"Models are the result of training an algorithm on a Snapshot of a dataset. They record metrics depending on their projects task, and are scored accordingly. Some models are the result of a hyperparameter search, and include additional analysis on the range of hyperparameters they are tested against.","title":"Models"},{"location":"user_guides/dashboard/overview/#snapshots","text":"A Snapshot is created during training runs to record the data used for further analysis, or to train additional models against identical data.","title":"Snapshots"},{"location":"user_guides/dashboard/overview/#deployments","text":"Every deployment is recorded to track models over time.","title":"Deployments"},{"location":"user_guides/predictions/deployments/","text":"Deployments \u00b6 Models are automatically deployed if their key metric ( R 2 for regression, F 1 for classification) is improved over the currently deployed version during training. If you want to manage deploys manually, you can always change which model is currently responsible for making predictions. API \u00b6 pgml.deploy 1 2 3 4 5 pgml . deploy ( project_name TEXT , -- Human-friendly project name strategy TEXT DEFAULT 'best_score' , -- 'rollback', 'best_score', or 'most_recent' algorithm_name TEXT DEFAULT NULL -- filter candidates to a particular algorithm, NULL = all qualify ) The default behavior allows any algorithm to qualify, but deployment candidates can be further restricted to a specific algorithm by passing the algorithm_name . Note Deployed models are cached at the session level to improve prediction times. Active sessions will not see deploys until they reconnect. SQL Output 1 SELECT * FROM pgml . deploy ( 'Handwritten Digit Image Classifier' , 'best_score' ); 1 2 3 4 project_name | strategy | algorithm_name ------------------------------------+----------------+---------------- Handwritten Digit Image Classifier | classification | linear ( 1 row ) Strategies \u00b6 There are 3 different deployment strategies available strategy description most_recent The most recently trained model for this project best_score The model that achieved the best key metric score rollback The model that was previously deployed for this project Rolling back to a specific algorithm \u00b6 Rolling back creates a new deploy for the model that was deployed before the current one. Multiple rollbacks in a row will effectively oscilate between the two most recently deployed models, making rollbacks a relatively safe operation. SQL Output 1 SELECT * FROM pgml . deploy ( 'Handwritten Digit Image Classifier' , 'rollback' , 'svm' ); 1 2 3 4 project_name | strategy | algorithm_name ------------------------------------+----------------+---------------- Handwritten Digit Image Classifier | classification | linear ( 1 row )","title":"Deploying Models"},{"location":"user_guides/predictions/deployments/#deployments","text":"Models are automatically deployed if their key metric ( R 2 for regression, F 1 for classification) is improved over the currently deployed version during training. If you want to manage deploys manually, you can always change which model is currently responsible for making predictions.","title":"Deployments"},{"location":"user_guides/predictions/deployments/#api","text":"pgml.deploy 1 2 3 4 5 pgml . deploy ( project_name TEXT , -- Human-friendly project name strategy TEXT DEFAULT 'best_score' , -- 'rollback', 'best_score', or 'most_recent' algorithm_name TEXT DEFAULT NULL -- filter candidates to a particular algorithm, NULL = all qualify ) The default behavior allows any algorithm to qualify, but deployment candidates can be further restricted to a specific algorithm by passing the algorithm_name . Note Deployed models are cached at the session level to improve prediction times. Active sessions will not see deploys until they reconnect. SQL Output 1 SELECT * FROM pgml . deploy ( 'Handwritten Digit Image Classifier' , 'best_score' ); 1 2 3 4 project_name | strategy | algorithm_name ------------------------------------+----------------+---------------- Handwritten Digit Image Classifier | classification | linear ( 1 row )","title":"API"},{"location":"user_guides/predictions/deployments/#strategies","text":"There are 3 different deployment strategies available strategy description most_recent The most recently trained model for this project best_score The model that achieved the best key metric score rollback The model that was previously deployed for this project","title":"Strategies"},{"location":"user_guides/predictions/deployments/#rolling-back-to-a-specific-algorithm","text":"Rolling back creates a new deploy for the model that was deployed before the current one. Multiple rollbacks in a row will effectively oscilate between the two most recently deployed models, making rollbacks a relatively safe operation. SQL Output 1 SELECT * FROM pgml . deploy ( 'Handwritten Digit Image Classifier' , 'rollback' , 'svm' ); 1 2 3 4 project_name | strategy | algorithm_name ------------------------------------+----------------+---------------- Handwritten Digit Image Classifier | classification | linear ( 1 row )","title":"Rolling back to a specific algorithm"},{"location":"user_guides/predictions/overview/","text":"Predictions \u00b6 The predict function is the key value proposition of PostgresML. It provides online predictions using the actively deployed model for a project. API \u00b6 pgml.predict 1 2 3 4 pgml . predict ( project_name TEXT , -- Human-friendly project name features DOUBLE PRECISION [] -- Must match the training data column order ) Example Once a model has been trained for a project, making predictions is as simple as: 1 2 3 4 SELECT pgml . predict ( 'Human-friendly project name' , ARRAY [...] ) AS prediction_score ; where ARRAY[...] is the same list of features for a sample used in training. This score can be used in normal queries, for example: Example 1 2 3 4 5 6 7 8 9 SELECT * , pgml . predict ( 'Probability of buying our products' , ARRAY [ user . location , NOW () - user . created_at , user . total_purchases_in_dollars ] ) AS likely_to_buy_score FROM users WHERE comapany_id = 5 ORDER BY likely_to_buy_score LIMIT 25 ; Making Predictions \u00b6 If you've already been through the training guide , you can see the results of those efforts: SQL Output 1 2 3 SELECT target , pgml . predict ( 'Handwritten Digit Image Classifier' , image ) AS prediction FROM pgml . digits LIMIT 10 ; 1 2 3 4 5 6 7 8 9 10 11 12 13 target | prediction --------+------------ 0 | 0 1 | 1 2 | 2 3 | 3 4 | 4 5 | 5 6 | 6 7 | 7 8 | 8 9 | 9 ( 10 rows ) Checking the deployed algorithm \u00b6 If you're ever curious about which deployed models will be used to make predictions, you can see them in the pgml.deployed_models VIEW. SQL Output 1 SELECT * FROM pgml . deployed_models ; 1 2 3 id | name | task | algorithm_name | deployed_at ----+------------------------------------+----------------+----------------+---------------------------- 1 | Handwritten Digit Image Classifier | classification | linear | 2022 - 05 - 10 15 : 28 : 53 . 383893","title":"Prediction Overview"},{"location":"user_guides/predictions/overview/#predictions","text":"The predict function is the key value proposition of PostgresML. It provides online predictions using the actively deployed model for a project.","title":"Predictions"},{"location":"user_guides/predictions/overview/#api","text":"pgml.predict 1 2 3 4 pgml . predict ( project_name TEXT , -- Human-friendly project name features DOUBLE PRECISION [] -- Must match the training data column order ) Example Once a model has been trained for a project, making predictions is as simple as: 1 2 3 4 SELECT pgml . predict ( 'Human-friendly project name' , ARRAY [...] ) AS prediction_score ; where ARRAY[...] is the same list of features for a sample used in training. This score can be used in normal queries, for example: Example 1 2 3 4 5 6 7 8 9 SELECT * , pgml . predict ( 'Probability of buying our products' , ARRAY [ user . location , NOW () - user . created_at , user . total_purchases_in_dollars ] ) AS likely_to_buy_score FROM users WHERE comapany_id = 5 ORDER BY likely_to_buy_score LIMIT 25 ;","title":"API"},{"location":"user_guides/predictions/overview/#making-predictions","text":"If you've already been through the training guide , you can see the results of those efforts: SQL Output 1 2 3 SELECT target , pgml . predict ( 'Handwritten Digit Image Classifier' , image ) AS prediction FROM pgml . digits LIMIT 10 ; 1 2 3 4 5 6 7 8 9 10 11 12 13 target | prediction --------+------------ 0 | 0 1 | 1 2 | 2 3 | 3 4 | 4 5 | 5 6 | 6 7 | 7 8 | 8 9 | 9 ( 10 rows )","title":"Making Predictions"},{"location":"user_guides/predictions/overview/#checking-the-deployed-algorithm","text":"If you're ever curious about which deployed models will be used to make predictions, you can see them in the pgml.deployed_models VIEW. SQL Output 1 SELECT * FROM pgml . deployed_models ; 1 2 3 id | name | task | algorithm_name | deployed_at ----+------------------------------------+----------------+----------------+---------------------------- 1 | Handwritten Digit Image Classifier | classification | linear | 2022 - 05 - 10 15 : 28 : 53 . 383893","title":"Checking the deployed algorithm"},{"location":"user_guides/schema/deployments/","text":"Deployments \u00b6 Deployments are an artifact of calls to pgml.deploy . See deployments for ways to create new deployments. Schema \u00b6 1 2 3 4 5 6 7 8 9 pgml . deployments ( id BIGSERIAL PRIMARY KEY , project_id BIGINT NOT NULL , model_id BIGINT NOT NULL , strategy TEXT NOT NULL , created_at TIMESTAMP WITHOUT TIME ZONE NOT NULL DEFAULT clock_timestamp (), CONSTRAINT project_id_fk FOREIGN KEY ( project_id ) REFERENCES pgml . projects ( id ), CONSTRAINT model_id_fk FOREIGN KEY ( model_id ) REFERENCES pgml . models ( id ) );","title":"Deployments"},{"location":"user_guides/schema/deployments/#deployments","text":"Deployments are an artifact of calls to pgml.deploy . See deployments for ways to create new deployments.","title":"Deployments"},{"location":"user_guides/schema/deployments/#schema","text":"1 2 3 4 5 6 7 8 9 pgml . deployments ( id BIGSERIAL PRIMARY KEY , project_id BIGINT NOT NULL , model_id BIGINT NOT NULL , strategy TEXT NOT NULL , created_at TIMESTAMP WITHOUT TIME ZONE NOT NULL DEFAULT clock_timestamp (), CONSTRAINT project_id_fk FOREIGN KEY ( project_id ) REFERENCES pgml . projects ( id ), CONSTRAINT model_id_fk FOREIGN KEY ( model_id ) REFERENCES pgml . models ( id ) );","title":"Schema"},{"location":"user_guides/schema/models/","text":"Models \u00b6 Models are an artifact of calls to pgml.train . See training for ways to create new models. Schema \u00b6 pgml.models 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 pgml . models ( id BIGSERIAL PRIMARY KEY , project_id BIGINT NOT NULL , snapshot_id BIGINT NOT NULL , algorithm_name TEXT NOT NULL , hyperparams JSONB NOT NULL , status TEXT NOT NULL , search TEXT , search_params JSONB NOT NULL , search_args JSONB NOT NULL , created_at TIMESTAMP WITHOUT TIME ZONE NOT NULL DEFAULT clock_timestamp (), updated_at TIMESTAMP WITHOUT TIME ZONE NOT NULL DEFAULT clock_timestamp (), metrics JSONB , CONSTRAINT project_id_fk FOREIGN KEY ( project_id ) REFERENCES pgml . projects ( id ), CONSTRAINT snapshot_id_fk FOREIGN KEY ( snapshot_id ) REFERENCES pgml . snapshots ( id ) );","title":"Models"},{"location":"user_guides/schema/models/#models","text":"Models are an artifact of calls to pgml.train . See training for ways to create new models.","title":"Models"},{"location":"user_guides/schema/models/#schema","text":"pgml.models 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 pgml . models ( id BIGSERIAL PRIMARY KEY , project_id BIGINT NOT NULL , snapshot_id BIGINT NOT NULL , algorithm_name TEXT NOT NULL , hyperparams JSONB NOT NULL , status TEXT NOT NULL , search TEXT , search_params JSONB NOT NULL , search_args JSONB NOT NULL , created_at TIMESTAMP WITHOUT TIME ZONE NOT NULL DEFAULT clock_timestamp (), updated_at TIMESTAMP WITHOUT TIME ZONE NOT NULL DEFAULT clock_timestamp (), metrics JSONB , CONSTRAINT project_id_fk FOREIGN KEY ( project_id ) REFERENCES pgml . projects ( id ), CONSTRAINT snapshot_id_fk FOREIGN KEY ( snapshot_id ) REFERENCES pgml . snapshots ( id ) );","title":"Schema"},{"location":"user_guides/schema/projects/","text":"Projects \u00b6 Projects are an artifact of calls to pgml.train . See training for ways to create new projects. Schema \u00b6 pgml.projects 1 2 3 4 5 6 7 pgml . projects ( id BIGSERIAL PRIMARY KEY , name TEXT NOT NULL , task TEXT NOT NULL , created_at TIMESTAMP WITHOUT TIME ZONE NOT NULL DEFAULT clock_timestamp (), updated_at TIMESTAMP WITHOUT TIME ZONE NOT NULL DEFAULT clock_timestamp () );","title":"Projects"},{"location":"user_guides/schema/projects/#projects","text":"Projects are an artifact of calls to pgml.train . See training for ways to create new projects.","title":"Projects"},{"location":"user_guides/schema/projects/#schema","text":"pgml.projects 1 2 3 4 5 6 7 pgml . projects ( id BIGSERIAL PRIMARY KEY , name TEXT NOT NULL , task TEXT NOT NULL , created_at TIMESTAMP WITHOUT TIME ZONE NOT NULL DEFAULT clock_timestamp (), updated_at TIMESTAMP WITHOUT TIME ZONE NOT NULL DEFAULT clock_timestamp () );","title":"Schema"},{"location":"user_guides/schema/snapshots/","text":"Snapshots \u00b6 Snapshots are an artifact of calls to pgml.train that specify the relation_name. See training for ways to create new snapshots. Schema \u00b6 pgml.snapshots 1 2 3 4 5 6 7 8 9 10 11 12 pgml . snapshots ( id BIGSERIAL PRIMARY KEY , relation_name TEXT NOT NULL , y_column_name TEXT [] NOT NULL , test_size FLOAT4 NOT NULL , test_sampling TEXT NOT NULL , status TEXT NOT NULL , columns JSONB , analysis JSONB , created_at TIMESTAMP WITHOUT TIME ZONE NOT NULL DEFAULT clock_timestamp (), updated_at TIMESTAMP WITHOUT TIME ZONE NOT NULL DEFAULT clock_timestamp () ); Tables \u00b6 Every snapshot has an accompaning table in the pgml schema. For example, the Snapshot with id = 42 has all data recorded in the table pgml.snaphot_42 . If the test_sampling was random for the training, the rows in the table were ORDER BY random() when it was created so that future samples can be consistently and efficiently randomized.","title":"Snapshots"},{"location":"user_guides/schema/snapshots/#snapshots","text":"Snapshots are an artifact of calls to pgml.train that specify the relation_name. See training for ways to create new snapshots.","title":"Snapshots"},{"location":"user_guides/schema/snapshots/#schema","text":"pgml.snapshots 1 2 3 4 5 6 7 8 9 10 11 12 pgml . snapshots ( id BIGSERIAL PRIMARY KEY , relation_name TEXT NOT NULL , y_column_name TEXT [] NOT NULL , test_size FLOAT4 NOT NULL , test_sampling TEXT NOT NULL , status TEXT NOT NULL , columns JSONB , analysis JSONB , created_at TIMESTAMP WITHOUT TIME ZONE NOT NULL DEFAULT clock_timestamp (), updated_at TIMESTAMP WITHOUT TIME ZONE NOT NULL DEFAULT clock_timestamp () );","title":"Schema"},{"location":"user_guides/schema/snapshots/#tables","text":"Every snapshot has an accompaning table in the pgml schema. For example, the Snapshot with id = 42 has all data recorded in the table pgml.snaphot_42 . If the test_sampling was random for the training, the rows in the table were ORDER BY random() when it was created so that future samples can be consistently and efficiently randomized.","title":"Tables"},{"location":"user_guides/setup/distributed_training/","text":"Distributed Training \u00b6 Depending on the size of your dataset and its change frequency, you may want to offload training (or inference) to secondary PostgreSQL servers to avoid excessive load on your primary. We've outlined several of the built-in mechanisms below to help distribute the load. pg_dump (< 10GB) \u00b6 pg_dump is a standard tool used to export data from a PostgreSQL database. If your dataset is small (e.g. less than 10GB) and changes infrequently, this could be quickest and simplest way to do it. Example # Export data from your production DB pg_dump \\ postgres://username:password@production-database.example.com/production_db \\ -t table_one \\ -t table_two > dump.sql # Import the data into PostgresML psql \\ postgres://username:password@postgresml.example.com/postgresml_db \\ -f dump.sql If you're using our Dockerized stack, you can import the data there: psql \\ postgres://postgres@localhost:5433/pgml_development \\ -f dump.sql PostgresML tables and functions are located in the pgml schema, so you can safely import your data into PostgresML without conflicts. You can also use pg_dump to copy the pgml schema to other servers which will make the trained models available in a distributed fashion. Foreign Data Wrappers (10GB - 100GB) \u00b6 Foreign Data Wrappers, or FDWs for short, are another good tool for reading or importing data from another PostgreSQL database into PostgresML. Setting up FDWs is a bit more involved than pg_dump but they provide real-time access to your production data and are good for small to medium size datasets (e.g. 10GB to 100GB) that change frequently. Official PostgreSQL docs explain FDWs with more detail; we'll document a basic example below. Install the extension \u00b6 PostgreSQL comes with postgres_fdw already available, but the extension needs to be explicitely installed into the database. Connect to your PostgresML database as a superuser and run: CREATE EXTENSION postgres_fdw ; Create foreign server \u00b6 A foreign server is a FDW reference to another PostgreSQL database running somewhere else. In this case, that foreign server is your production database. CREATE SERVER your_production_db FOREIGN DATA WRAPPER postgres_fdw OPTIONS ( host 'production-database.example.com' , port '5432' , dbname 'production_db' ); Create user mapping \u00b6 A user mapping is a relationship between the user you're connecting with to PostgresML and a user that exists on your production database. FDW will use this mapping to talk to your database when it wants to read some data. CREATE USER MAPPING FOR pgml_user SERVER your_production_db OPTIONS ( user 'your_production_db_user' , password 'your_production_db_user_password' ); At this point, when you connect to PostgresML using the example pgml_user and then query data in your production database using FDW, it'll use the user your_production_db_user to connect to your DB and fetch the data. Make sure that your_production_db_user has SELECT permissions on the tables you want to query and the USAGE permissions on the schema. Import the tables \u00b6 The final step is import your production database tables into PostgresML by creating a foreign schema mapping. This mapping will tell PostgresML which tables are available in your database. The quickest way is to import all of them, like so: IMPORT FOREIGN SCHEMA public FROM SERVER your_production_db INTO public ; This will import all tables from your production DB public schema into the public schema in PostgresML. The tables are now available for querying in PostgresML. Usage \u00b6 PostgresML snapshots the data before training on it, so every time you run pgml.train with a relation_name argument, the data will be fetched from the foreign data wrapper and imported into PostgresML. FDWs are reasonably good at fetching only the data specified by the VIEW , so if you place sufficient limits on your dataset in the CREATE VIEW statement, e.g. train on the last two weeks of data, or something similar, FDWs will do its best to fetch only the last two weeks of data in an efficient manner, leaving the rest behind on the primary. Logical replication (100GB - 10TB) \u00b6 Logical replication is a replication mechanism that's been available since PostgreSQL 10. It allows to copy entire tables and schemas from any database into PostgresML and keeping them up-to-date in real-time fairly cheaply as the data in production changes. This is suitable for medium to large PostgreSQL deployments (e.g. 100GB - 10TB). Logical replication is designed as a pub/sub system, where your production database is the publisher and PostgresML is the subscriber. As data in your database changes, it is streamed into PostgresML in milliseconds, which is very similar to how Postgres streaming replication works as well. The setup is slightly more involved than Foreign Data Wrappers, and is documented below. All queries must be run as a superuser. WAL \u00b6 First, make sure that your production DB has logical replication enabled. For this, it has to be on PostgreSQL 10 or above and also have wal_level configuration setting set to logical . SQL Output SHOW wal_level ; wal_level ----------- logical (1 row) If this is not the case, you'll need to change it and restart the server. Publication \u00b6 The publication is created on your production DB and configures which tables are replicated using logical replication. To replicate all tables in your public schema, you can run this: CREATE PUBLICATION all_tables FOR ALL TABLES ; Schema \u00b6 Logical replication does not copy the schema, so it needs to be copied manually in advance; pg_dump is great for this: # Dump the schema from your production DB pg_dump \\ postgres://username:password@production-db.example.com/production_db \\ --schema-only \\ --no-owner > schema.sql # Import the schema in PostgresML psql \\ postgres://username:password@postgresml.example.com/postgresml_db \\ -f schema.sql Subscription \u00b6 The subscription is created in your PostgresML database. To replicate all the tables we marked in the previous step, run: CREATE SUBSCRIPTION all_tables CONNECTION 'postgres://superuser:password@production-database.example.com/production_db' PUBLICATION all_tables ; As soon as you run this, logical replication will begin. It will start by copying all the data from your production database into PostgresML. That will take a while, depending on database size, network connection and hardware performance. Each table will be copied individually and the process is parallelized. Once the copy is complete, logical replication will synchronize and will replicate the data from your production database into PostgresML in real-time. Schema changes \u00b6 Logical replication has one notable limitation: it does not replicate schema (table) changes. If you change a table in your production DB in an incompatible way, e.g. by adding a column, the replication will break. To remediate this, when you're performing the schema change, make the change first in PostgresML and then in your production database. Native installation (10TB and beyond) \u00b6 For databases that are very large, e.g. 10TB+, we recommend you install the extension directly into your database. This option is available for databases of all sizes, but we recognize that many small to medium databases run on managed services, e.g. RDS, which don't allow this mechanism.","title":"Distributed Training"},{"location":"user_guides/setup/distributed_training/#distributed-training","text":"Depending on the size of your dataset and its change frequency, you may want to offload training (or inference) to secondary PostgreSQL servers to avoid excessive load on your primary. We've outlined several of the built-in mechanisms below to help distribute the load.","title":"Distributed Training"},{"location":"user_guides/setup/distributed_training/#pg_dump-10gb","text":"pg_dump is a standard tool used to export data from a PostgreSQL database. If your dataset is small (e.g. less than 10GB) and changes infrequently, this could be quickest and simplest way to do it. Example # Export data from your production DB pg_dump \\ postgres://username:password@production-database.example.com/production_db \\ -t table_one \\ -t table_two > dump.sql # Import the data into PostgresML psql \\ postgres://username:password@postgresml.example.com/postgresml_db \\ -f dump.sql If you're using our Dockerized stack, you can import the data there: psql \\ postgres://postgres@localhost:5433/pgml_development \\ -f dump.sql PostgresML tables and functions are located in the pgml schema, so you can safely import your data into PostgresML without conflicts. You can also use pg_dump to copy the pgml schema to other servers which will make the trained models available in a distributed fashion.","title":"pg_dump (&lt; 10GB)"},{"location":"user_guides/setup/distributed_training/#foreign-data-wrappers-10gb-100gb","text":"Foreign Data Wrappers, or FDWs for short, are another good tool for reading or importing data from another PostgreSQL database into PostgresML. Setting up FDWs is a bit more involved than pg_dump but they provide real-time access to your production data and are good for small to medium size datasets (e.g. 10GB to 100GB) that change frequently. Official PostgreSQL docs explain FDWs with more detail; we'll document a basic example below.","title":"Foreign Data Wrappers (10GB - 100GB)"},{"location":"user_guides/setup/distributed_training/#install-the-extension","text":"PostgreSQL comes with postgres_fdw already available, but the extension needs to be explicitely installed into the database. Connect to your PostgresML database as a superuser and run: CREATE EXTENSION postgres_fdw ;","title":"Install the extension"},{"location":"user_guides/setup/distributed_training/#create-foreign-server","text":"A foreign server is a FDW reference to another PostgreSQL database running somewhere else. In this case, that foreign server is your production database. CREATE SERVER your_production_db FOREIGN DATA WRAPPER postgres_fdw OPTIONS ( host 'production-database.example.com' , port '5432' , dbname 'production_db' );","title":"Create foreign server"},{"location":"user_guides/setup/distributed_training/#create-user-mapping","text":"A user mapping is a relationship between the user you're connecting with to PostgresML and a user that exists on your production database. FDW will use this mapping to talk to your database when it wants to read some data. CREATE USER MAPPING FOR pgml_user SERVER your_production_db OPTIONS ( user 'your_production_db_user' , password 'your_production_db_user_password' ); At this point, when you connect to PostgresML using the example pgml_user and then query data in your production database using FDW, it'll use the user your_production_db_user to connect to your DB and fetch the data. Make sure that your_production_db_user has SELECT permissions on the tables you want to query and the USAGE permissions on the schema.","title":"Create user mapping"},{"location":"user_guides/setup/distributed_training/#import-the-tables","text":"The final step is import your production database tables into PostgresML by creating a foreign schema mapping. This mapping will tell PostgresML which tables are available in your database. The quickest way is to import all of them, like so: IMPORT FOREIGN SCHEMA public FROM SERVER your_production_db INTO public ; This will import all tables from your production DB public schema into the public schema in PostgresML. The tables are now available for querying in PostgresML.","title":"Import the tables"},{"location":"user_guides/setup/distributed_training/#usage","text":"PostgresML snapshots the data before training on it, so every time you run pgml.train with a relation_name argument, the data will be fetched from the foreign data wrapper and imported into PostgresML. FDWs are reasonably good at fetching only the data specified by the VIEW , so if you place sufficient limits on your dataset in the CREATE VIEW statement, e.g. train on the last two weeks of data, or something similar, FDWs will do its best to fetch only the last two weeks of data in an efficient manner, leaving the rest behind on the primary.","title":"Usage"},{"location":"user_guides/setup/distributed_training/#logical-replication-100gb-10tb","text":"Logical replication is a replication mechanism that's been available since PostgreSQL 10. It allows to copy entire tables and schemas from any database into PostgresML and keeping them up-to-date in real-time fairly cheaply as the data in production changes. This is suitable for medium to large PostgreSQL deployments (e.g. 100GB - 10TB). Logical replication is designed as a pub/sub system, where your production database is the publisher and PostgresML is the subscriber. As data in your database changes, it is streamed into PostgresML in milliseconds, which is very similar to how Postgres streaming replication works as well. The setup is slightly more involved than Foreign Data Wrappers, and is documented below. All queries must be run as a superuser.","title":"Logical replication (100GB - 10TB)"},{"location":"user_guides/setup/distributed_training/#wal","text":"First, make sure that your production DB has logical replication enabled. For this, it has to be on PostgreSQL 10 or above and also have wal_level configuration setting set to logical . SQL Output SHOW wal_level ; wal_level ----------- logical (1 row) If this is not the case, you'll need to change it and restart the server.","title":"WAL"},{"location":"user_guides/setup/distributed_training/#publication","text":"The publication is created on your production DB and configures which tables are replicated using logical replication. To replicate all tables in your public schema, you can run this: CREATE PUBLICATION all_tables FOR ALL TABLES ;","title":"Publication"},{"location":"user_guides/setup/distributed_training/#schema","text":"Logical replication does not copy the schema, so it needs to be copied manually in advance; pg_dump is great for this: # Dump the schema from your production DB pg_dump \\ postgres://username:password@production-db.example.com/production_db \\ --schema-only \\ --no-owner > schema.sql # Import the schema in PostgresML psql \\ postgres://username:password@postgresml.example.com/postgresml_db \\ -f schema.sql","title":"Schema"},{"location":"user_guides/setup/distributed_training/#subscription","text":"The subscription is created in your PostgresML database. To replicate all the tables we marked in the previous step, run: CREATE SUBSCRIPTION all_tables CONNECTION 'postgres://superuser:password@production-database.example.com/production_db' PUBLICATION all_tables ; As soon as you run this, logical replication will begin. It will start by copying all the data from your production database into PostgresML. That will take a while, depending on database size, network connection and hardware performance. Each table will be copied individually and the process is parallelized. Once the copy is complete, logical replication will synchronize and will replicate the data from your production database into PostgresML in real-time.","title":"Subscription"},{"location":"user_guides/setup/distributed_training/#schema-changes","text":"Logical replication has one notable limitation: it does not replicate schema (table) changes. If you change a table in your production DB in an incompatible way, e.g. by adding a column, the replication will break. To remediate this, when you're performing the schema change, make the change first in PostgresML and then in your production database.","title":"Schema changes"},{"location":"user_guides/setup/distributed_training/#native-installation-10tb-and-beyond","text":"For databases that are very large, e.g. 10TB+, we recommend you install the extension directly into your database. This option is available for databases of all sizes, but we recognize that many small to medium databases run on managed services, e.g. RDS, which don't allow this mechanism.","title":"Native installation (10TB and beyond)"},{"location":"user_guides/setup/gpu_support/","text":"GPU Support \u00b6 PostgresML is capable of leveraging GPUs when the underlying libraries and hardware are properly configured on the database server. Tip Models trained on GPU will also require GPU support to make predictions. Tensorflow \u00b6 GPU setup for Tensorflow is covered in the documentation . You may acquire pre-trained GPU enabled models for fine tuning from Hugging Face . Torch \u00b6 GPU setup for Torch is covered in the documentation . You may acquire pre-trained GPU enabled models for fine tuning from Hugging Face . Flax \u00b6 GPU setup for Flax is covered in the documentation . You may acquire pre-trained GPU enabled models for fine tuning from Hugging Face . XGBoost \u00b6 GPU setup for XGBoost is covered in the documentation . Example 1 2 3 4 5 pgml . train ( 'GPU project' , algorithm => 'xgboost' , hyperparams => '{\"tree_method\" : \"gpu_hist\"}' ); LightGBM \u00b6 GPU setup for LightGBM is covered in the documentation . Example 1 2 3 4 5 pgml . train ( 'GPU project' , algorithm => 'lightgbm' , hyperparams => '{\"device\" : \"gpu\"}' ); Scikit-learn \u00b6 None of the scikit-learn algorithms natively support GPU devices. There are a few projects to improve scikit performance with additional parralellism, although we currently have not integrated these with PostgresML: https://github.com/intel/scikit-learn-intelex https://github.com/rapidsai/cuml If your project would benefit from GPU support, please consider opening an issue so we can prioritize integrations.","title":"GPU Support"},{"location":"user_guides/setup/gpu_support/#gpu-support","text":"PostgresML is capable of leveraging GPUs when the underlying libraries and hardware are properly configured on the database server. Tip Models trained on GPU will also require GPU support to make predictions.","title":"GPU Support"},{"location":"user_guides/setup/gpu_support/#tensorflow","text":"GPU setup for Tensorflow is covered in the documentation . You may acquire pre-trained GPU enabled models for fine tuning from Hugging Face .","title":"Tensorflow"},{"location":"user_guides/setup/gpu_support/#torch","text":"GPU setup for Torch is covered in the documentation . You may acquire pre-trained GPU enabled models for fine tuning from Hugging Face .","title":"Torch"},{"location":"user_guides/setup/gpu_support/#flax","text":"GPU setup for Flax is covered in the documentation . You may acquire pre-trained GPU enabled models for fine tuning from Hugging Face .","title":"Flax"},{"location":"user_guides/setup/gpu_support/#xgboost","text":"GPU setup for XGBoost is covered in the documentation . Example 1 2 3 4 5 pgml . train ( 'GPU project' , algorithm => 'xgboost' , hyperparams => '{\"tree_method\" : \"gpu_hist\"}' );","title":"XGBoost"},{"location":"user_guides/setup/gpu_support/#lightgbm","text":"GPU setup for LightGBM is covered in the documentation . Example 1 2 3 4 5 pgml . train ( 'GPU project' , algorithm => 'lightgbm' , hyperparams => '{\"device\" : \"gpu\"}' );","title":"LightGBM"},{"location":"user_guides/setup/gpu_support/#scikit-learn","text":"None of the scikit-learn algorithms natively support GPU devices. There are a few projects to improve scikit performance with additional parralellism, although we currently have not integrated these with PostgresML: https://github.com/intel/scikit-learn-intelex https://github.com/rapidsai/cuml If your project would benefit from GPU support, please consider opening an issue so we can prioritize integrations.","title":"Scikit-learn"},{"location":"user_guides/setup/native_installation/","text":"Native Installation \u00b6 A PostgresML deployment consists of two different runtimes. The foundational runtime is a Python extension for Postgres ( pgml-extension ) that facilitates the machine learning lifecycle inside the database. Additionally, we provide a dashboard ( pgml-dashboard ) that can connect to your Postgres server and provide additional management functionality. It will also provide visibility into the models you build and data they use. Install PostgreSQL with PL/Python \u00b6 PostgresML leverages Python libraries for their machine learning capabilities. You'll need to make sure the PostgreSQL installation has PL/Python built in. OS X Linux Windows We recommend you use Postgres.app because it comes with PL/Python . Otherwise, you'll need to install PL/Python manually. Once you have Postgres.app running, you'll need to install the Python framework. Mac OS has multiple distributions of Python, namely one from Brew and one from the Python community (Python.org); Postgres.app and PL/Python depend on the community one. The following versions of Python and Postgres.app are compatible: PostgreSQL version Python version Download link 14 3.9 Python 3.9 64-bit 13 3.8 Python 3.8 64-bit All Python.org installers for Mac OS are available here . You can also get more details about this in the Postgres.app documentation . Each Ubuntu/Debian distribution comes with its own version of PostgreSQL, the simplest way is to install it from Aptitude: $ sudo apt-get install -y postgresql-plpython3-12 python3 python3-pip postgresql-12 EnterpriseDB provides Windows builds of PostgreSQL available for download . Install the extension \u00b6 To use our Python package inside PostgreSQL, we need to install it into the global Python package space. Depending on which version of Python you installed in the previous step, use the corresponding pip executable. Change the --database-url option to point to your PostgreSQL server. $ sudo pip3 install pgml-extension $ python3 -m pgml_extension --database-url = postgres://user_name:password@localhost:5432/database_name If everything works, you should be able to run this successfully: $ psql -c 'SELECT pgml.version()' postgres://user_name:password@localhost:5432/database_name Run the dashboard \u00b6 The PostgresML dashboard is a Django app, that can be run against any PostgreSQL installation. There is an included Dockerfile if you wish to run it as a container, or you may want to setup a Python venv to isolate the dependencies. Basic install can be achieved with: Clone the repo: $ git clone git@github.com:postgresml/postgresml.git && cd postgresml/pgml-dashboard Set your PGML_DATABASE_URL environment variable: $ echo PGML_DATABASE_URL = postgres://user_name:password@localhost:5432/database_name > .env Install dependencies: $ pip install -r requirements.txt Run the server: $ python manage.py runserver","title":"Native Installation"},{"location":"user_guides/setup/native_installation/#native-installation","text":"A PostgresML deployment consists of two different runtimes. The foundational runtime is a Python extension for Postgres ( pgml-extension ) that facilitates the machine learning lifecycle inside the database. Additionally, we provide a dashboard ( pgml-dashboard ) that can connect to your Postgres server and provide additional management functionality. It will also provide visibility into the models you build and data they use.","title":"Native Installation"},{"location":"user_guides/setup/native_installation/#install-postgresql-with-plpython","text":"PostgresML leverages Python libraries for their machine learning capabilities. You'll need to make sure the PostgreSQL installation has PL/Python built in. OS X Linux Windows We recommend you use Postgres.app because it comes with PL/Python . Otherwise, you'll need to install PL/Python manually. Once you have Postgres.app running, you'll need to install the Python framework. Mac OS has multiple distributions of Python, namely one from Brew and one from the Python community (Python.org); Postgres.app and PL/Python depend on the community one. The following versions of Python and Postgres.app are compatible: PostgreSQL version Python version Download link 14 3.9 Python 3.9 64-bit 13 3.8 Python 3.8 64-bit All Python.org installers for Mac OS are available here . You can also get more details about this in the Postgres.app documentation . Each Ubuntu/Debian distribution comes with its own version of PostgreSQL, the simplest way is to install it from Aptitude: $ sudo apt-get install -y postgresql-plpython3-12 python3 python3-pip postgresql-12 EnterpriseDB provides Windows builds of PostgreSQL available for download .","title":"Install PostgreSQL with PL/Python"},{"location":"user_guides/setup/native_installation/#install-the-extension","text":"To use our Python package inside PostgreSQL, we need to install it into the global Python package space. Depending on which version of Python you installed in the previous step, use the corresponding pip executable. Change the --database-url option to point to your PostgreSQL server. $ sudo pip3 install pgml-extension $ python3 -m pgml_extension --database-url = postgres://user_name:password@localhost:5432/database_name If everything works, you should be able to run this successfully: $ psql -c 'SELECT pgml.version()' postgres://user_name:password@localhost:5432/database_name","title":"Install the extension"},{"location":"user_guides/setup/native_installation/#run-the-dashboard","text":"The PostgresML dashboard is a Django app, that can be run against any PostgreSQL installation. There is an included Dockerfile if you wish to run it as a container, or you may want to setup a Python venv to isolate the dependencies. Basic install can be achieved with: Clone the repo: $ git clone git@github.com:postgresml/postgresml.git && cd postgresml/pgml-dashboard Set your PGML_DATABASE_URL environment variable: $ echo PGML_DATABASE_URL = postgres://user_name:password@localhost:5432/database_name > .env Install dependencies: $ pip install -r requirements.txt Run the server: $ python manage.py runserver","title":"Run the dashboard"},{"location":"user_guides/setup/quick_start_with_docker/","text":"Quick Start w/ Docker \u00b6 We've prebuilt docker images for common operating systems that will allow you to quickly spin up a new PostgreSQL server with PL/Python installed, along with the PostgresML extension. This database is seeded with a few toy datasets so you can experiment with a PostgresML workflow and quickly see results in the dashboard without needing to bring your own data. You can skip to Native Installation if you're ready to start using your own data in a native PostgreSQL installation. OS X Linux Windows Install Docker for OS X . Install Docker for Linux . Some package managers (e.g. Ubuntu/Debian) additionally require the docker-compose package to be installed separately. Install Docker for Windows . Use the Linux instructions if you're installing in Windows Subsystem for Linux. Clone the repo: $ git clone git@github.com:postgresml/postgresml.git Start dockerized services. PostgresML will run on port 5433, just in case you already have Postgres running: $ cd postgresml && docker-compose up Connect to Postgres in the container with PostgresML installed: $ psql postgres://postgres@localhost:5433/pgml_development Validate your installation: pgml_development =# SELECT pgml . version (); version --------- 0 . 8 . 1 ( 1 row ) Docker Compose will also start the dashboard app running locally http://localhost:8000/","title":"Quick Start with Docker"},{"location":"user_guides/setup/quick_start_with_docker/#quick-start-w-docker","text":"We've prebuilt docker images for common operating systems that will allow you to quickly spin up a new PostgreSQL server with PL/Python installed, along with the PostgresML extension. This database is seeded with a few toy datasets so you can experiment with a PostgresML workflow and quickly see results in the dashboard without needing to bring your own data. You can skip to Native Installation if you're ready to start using your own data in a native PostgreSQL installation. OS X Linux Windows Install Docker for OS X . Install Docker for Linux . Some package managers (e.g. Ubuntu/Debian) additionally require the docker-compose package to be installed separately. Install Docker for Windows . Use the Linux instructions if you're installing in Windows Subsystem for Linux. Clone the repo: $ git clone git@github.com:postgresml/postgresml.git Start dockerized services. PostgresML will run on port 5433, just in case you already have Postgres running: $ cd postgresml && docker-compose up Connect to Postgres in the container with PostgresML installed: $ psql postgres://postgres@localhost:5433/pgml_development Validate your installation: pgml_development =# SELECT pgml . version (); version --------- 0 . 8 . 1 ( 1 row ) Docker Compose will also start the dashboard app running locally http://localhost:8000/","title":"Quick Start w/ Docker"},{"location":"user_guides/training/algorithm_selection/","text":"Algorithms \u00b6 We currently support the following regression and classification algorithms from Scikit-Learn , XGBoost , and LightGBM . You may pass any of these to the pgml.train(algorithm => ...) argument. The hyperparams argument will pass parameters on. See the associated documentation for valid hyperparameters of each algorithm. Tip Try experimenting with multiple algorithms to explore their performance characteristics on your dataset. It's often hard to predict exactly which algorithm will be the best, but once you've prepared your training data, it can be efficiently reused by PostgresML. pgml.train creates a Snapshot each time it is called with the relation_name argument. You can reuse identical data across multiple runs by omitting that, and passing a different algorithm argument instead. The PostgresML dashboard makes it easy to compare various algorithms on your dataset. Gradient Boosting \u00b6 Algorithm Regression Classification xgboost XGBRegressor XGBClassifier xgboost_random_forest XGBRFRegressor XGBRFClassifier lightgbm LGBMRegressor LGBMClassifier Scikit Ensembles \u00b6 Algorithm Regression Classification ada_boost AdaBoostRegressor AdaBoostClassifier bagging BaggingRegressor BaggingClassifier extra_trees ExtraTreesRegressor ExtraTreesClassifier gradient_boosting_trees GradientBoostingRegressor GradientBoostingClassifier random_forest RandomForestRegressor RandomForestClassifier hist_gradient_boosting HistGradientBoostingRegressor HistGradientBoostingClassifier Support Vector Machines \u00b6 Algorithm Regression Classification svm SVR SVC nu_svm NuSVR NuSVC linear_svm LinearSVR LinearSVC Linear Models \u00b6 Algorithm Regression Classification linear LinearRegression LogisticRegression ridge Ridge RidgeClassifier lasso Lasso - elastic_net ElasticNet - least_angle LARS - lasso_least_angle LassoLars - orthoganl_matching_pursuit OrthogonalMatchingPursuit - bayesian_ridge BayesianRidge - automatic_relevance_determination ARDRegression - stochastic_gradient_descent SGDRegressor SGDClassifier perceptron - Perceptron passive_aggressive PassiveAggressiveRegressor PassiveAggressiveClassifier ransac RANSACRegressor - theil_sen TheilSenRegressor - huber HuberRegressor - quantile QuantileRegressor - Other \u00b6 Algorithm Regression Classification kernel_ridge KernelRidge - gaussian_process GaussianProcessRegressor GaussianProcessClassifier","title":"Algorithm Selection"},{"location":"user_guides/training/algorithm_selection/#algorithms","text":"We currently support the following regression and classification algorithms from Scikit-Learn , XGBoost , and LightGBM . You may pass any of these to the pgml.train(algorithm => ...) argument. The hyperparams argument will pass parameters on. See the associated documentation for valid hyperparameters of each algorithm. Tip Try experimenting with multiple algorithms to explore their performance characteristics on your dataset. It's often hard to predict exactly which algorithm will be the best, but once you've prepared your training data, it can be efficiently reused by PostgresML. pgml.train creates a Snapshot each time it is called with the relation_name argument. You can reuse identical data across multiple runs by omitting that, and passing a different algorithm argument instead. The PostgresML dashboard makes it easy to compare various algorithms on your dataset.","title":"Algorithms"},{"location":"user_guides/training/algorithm_selection/#gradient-boosting","text":"Algorithm Regression Classification xgboost XGBRegressor XGBClassifier xgboost_random_forest XGBRFRegressor XGBRFClassifier lightgbm LGBMRegressor LGBMClassifier","title":"Gradient Boosting"},{"location":"user_guides/training/algorithm_selection/#scikit-ensembles","text":"Algorithm Regression Classification ada_boost AdaBoostRegressor AdaBoostClassifier bagging BaggingRegressor BaggingClassifier extra_trees ExtraTreesRegressor ExtraTreesClassifier gradient_boosting_trees GradientBoostingRegressor GradientBoostingClassifier random_forest RandomForestRegressor RandomForestClassifier hist_gradient_boosting HistGradientBoostingRegressor HistGradientBoostingClassifier","title":"Scikit Ensembles"},{"location":"user_guides/training/algorithm_selection/#support-vector-machines","text":"Algorithm Regression Classification svm SVR SVC nu_svm NuSVR NuSVC linear_svm LinearSVR LinearSVC","title":"Support Vector Machines"},{"location":"user_guides/training/algorithm_selection/#linear-models","text":"Algorithm Regression Classification linear LinearRegression LogisticRegression ridge Ridge RidgeClassifier lasso Lasso - elastic_net ElasticNet - least_angle LARS - lasso_least_angle LassoLars - orthoganl_matching_pursuit OrthogonalMatchingPursuit - bayesian_ridge BayesianRidge - automatic_relevance_determination ARDRegression - stochastic_gradient_descent SGDRegressor SGDClassifier perceptron - Perceptron passive_aggressive PassiveAggressiveRegressor PassiveAggressiveClassifier ransac RANSACRegressor - theil_sen TheilSenRegressor - huber HuberRegressor - quantile QuantileRegressor -","title":"Linear Models"},{"location":"user_guides/training/algorithm_selection/#other","text":"Algorithm Regression Classification kernel_ridge KernelRidge - gaussian_process GaussianProcessRegressor GaussianProcessClassifier","title":"Other"},{"location":"user_guides/training/hyperparameter_search/","text":"Hyperparameter Search \u00b6 Models can be further refined with the scikit cross validation hyperparameter search libraries. We currently support the grid and random implementations. Visual Analysis \u00b6 The optimal set of hyperparams will be chosen for the model, and that combination is highlighted in the dashboard among all search candidates. The impact of each hyperparameter is measured against the key metric, as well as the training and test times. In this particular case, it's interesting that as max_depth increases, the \"Test Score\" on the key metric trends lower, so the smallest value of max_depth is chosen to maximize the \"Test Score\". Luckily, the smallest max_depth values also have the fastest \"Fit Time\", indicating that we pay less for training these higher quality models. It's a little less obvious how the different values n_estimators and learning_rate impact the test score. We may want to rerun our search and zoom in our out in the search space to get more insight. API \u00b6 The arguments to pgml.train that begin with search are used for hyperparameter tuning. pgml.train 1 2 3 4 5 6 7 8 9 10 11 12 13 pgml . train ( project_name TEXT , -- Human-friendly project name task TEXT DEFAULT NULL , -- 'regression' or 'classification' relation_name TEXT DEFAULT NULL , -- name of table or view y_column_name TEXT DEFAULT NULL , -- aka \"label\" or \"unknown\" or \"target\" algorithm TEXT DEFAULT 'linear' , -- statistical learning method hyperparams JSONB DEFAULT '{}' :: JSONB , -- options for the model search TEXT DEFAULT NULL , -- hyperparam tuning, 'grid' or 'random' search_params JSONB DEFAULT '{}' :: JSONB , -- hyperparam search space search_args JSONB DEFAULT '{}' :: JSONB , -- hyperparam options test_size REAL DEFAULT 0 . 25 , -- fraction of the data for the test set test_sampling TEXT DEFAULT 'random' -- 'random', 'first' or 'last' ) search can either be grid or random . search_params is the set of hyperparameters to search for your algorithm search_args are passed to the scikit learn model selection algorithm for extra configuration search description grid Trains every permutation of search_params random Randomly samples search_params to train models You may pass any of the arguments listed in the algorithms documentation as hyperparameters. See Algorithms for the complete list of algorithms and their associated documentation. Example \u00b6 This grid search will train len(max_depth) * len(n_estimators) * len(learning_rate) = 6 * 4 * 4 = 96 combinations to compare all possible permutations of the search_params . It takes a couple of minutes on my computer, but you can delete some values if you want to speed things up. I like to watch all cores operate at 100% utilization in a separate terminal with htop . SQL Output 1 2 3 4 5 6 7 8 9 10 SELECT * FROM pgml . train ( 'Handwritten Digit Image Classifier' , algorithm => 'xgboost' , search => 'grid' , search_params => '{ \"max_depth\": [1, 2, 3, 4, 5, 6], \"n_estimators\": [20, 40, 80, 160], \"learning_rate\": [0.1, 0.2, 0.3, 0.4] }' ); 1 2 3 4 project_name | task | algorithm_name | status ------------------------------------+-----------+----------------+---------- Handwritten Digit Image Classifier | | xgboost | deployed ( 1 row ) As you can see from the output, a new set model has been deployed with a better performance. There will also be a new analysis available on this model visible in the dashboard.","title":"Hyperparameter Search"},{"location":"user_guides/training/hyperparameter_search/#hyperparameter-search","text":"Models can be further refined with the scikit cross validation hyperparameter search libraries. We currently support the grid and random implementations.","title":"Hyperparameter Search"},{"location":"user_guides/training/hyperparameter_search/#visual-analysis","text":"The optimal set of hyperparams will be chosen for the model, and that combination is highlighted in the dashboard among all search candidates. The impact of each hyperparameter is measured against the key metric, as well as the training and test times. In this particular case, it's interesting that as max_depth increases, the \"Test Score\" on the key metric trends lower, so the smallest value of max_depth is chosen to maximize the \"Test Score\". Luckily, the smallest max_depth values also have the fastest \"Fit Time\", indicating that we pay less for training these higher quality models. It's a little less obvious how the different values n_estimators and learning_rate impact the test score. We may want to rerun our search and zoom in our out in the search space to get more insight.","title":"Visual Analysis"},{"location":"user_guides/training/hyperparameter_search/#api","text":"The arguments to pgml.train that begin with search are used for hyperparameter tuning. pgml.train 1 2 3 4 5 6 7 8 9 10 11 12 13 pgml . train ( project_name TEXT , -- Human-friendly project name task TEXT DEFAULT NULL , -- 'regression' or 'classification' relation_name TEXT DEFAULT NULL , -- name of table or view y_column_name TEXT DEFAULT NULL , -- aka \"label\" or \"unknown\" or \"target\" algorithm TEXT DEFAULT 'linear' , -- statistical learning method hyperparams JSONB DEFAULT '{}' :: JSONB , -- options for the model search TEXT DEFAULT NULL , -- hyperparam tuning, 'grid' or 'random' search_params JSONB DEFAULT '{}' :: JSONB , -- hyperparam search space search_args JSONB DEFAULT '{}' :: JSONB , -- hyperparam options test_size REAL DEFAULT 0 . 25 , -- fraction of the data for the test set test_sampling TEXT DEFAULT 'random' -- 'random', 'first' or 'last' ) search can either be grid or random . search_params is the set of hyperparameters to search for your algorithm search_args are passed to the scikit learn model selection algorithm for extra configuration search description grid Trains every permutation of search_params random Randomly samples search_params to train models You may pass any of the arguments listed in the algorithms documentation as hyperparameters. See Algorithms for the complete list of algorithms and their associated documentation.","title":"API"},{"location":"user_guides/training/hyperparameter_search/#example","text":"This grid search will train len(max_depth) * len(n_estimators) * len(learning_rate) = 6 * 4 * 4 = 96 combinations to compare all possible permutations of the search_params . It takes a couple of minutes on my computer, but you can delete some values if you want to speed things up. I like to watch all cores operate at 100% utilization in a separate terminal with htop . SQL Output 1 2 3 4 5 6 7 8 9 10 SELECT * FROM pgml . train ( 'Handwritten Digit Image Classifier' , algorithm => 'xgboost' , search => 'grid' , search_params => '{ \"max_depth\": [1, 2, 3, 4, 5, 6], \"n_estimators\": [20, 40, 80, 160], \"learning_rate\": [0.1, 0.2, 0.3, 0.4] }' ); 1 2 3 4 project_name | task | algorithm_name | status ------------------------------------+-----------+----------------+---------- Handwritten Digit Image Classifier | | xgboost | deployed ( 1 row ) As you can see from the output, a new set model has been deployed with a better performance. There will also be a new analysis available on this model visible in the dashboard.","title":"Example"},{"location":"user_guides/training/joint_optimization/","text":"Joint Optimization \u00b6 Some algorithms support joint optimization of the task across multiple outputs, and can improve results compared to using multiple independent models. To leverage multiple outputs in PostgresML, you'll need to substitue the standard usage of pgml.train and pgml.predict with pgml.train_joint and pgml.predict_joint . The _joint functions are identical, except train_joint takes an array of y_column_names TEXT[] , and predict_joint returns an array of outputs correspondingly. Read more at scikit-learn .","title":"Joint Optimization"},{"location":"user_guides/training/joint_optimization/#joint-optimization","text":"Some algorithms support joint optimization of the task across multiple outputs, and can improve results compared to using multiple independent models. To leverage multiple outputs in PostgresML, you'll need to substitue the standard usage of pgml.train and pgml.predict with pgml.train_joint and pgml.predict_joint . The _joint functions are identical, except train_joint takes an array of y_column_names TEXT[] , and predict_joint returns an array of outputs correspondingly. Read more at scikit-learn .","title":"Joint Optimization"},{"location":"user_guides/training/overview/","text":"Training \u00b6 The training function is at the heart of PostgresML. It's a powerful single call that can handle the different tasks of training depending on the arguments passed. API \u00b6 Most parameters are optional other than the project_name which is a simple human readable identifier to organize your work. pgml.train 1 2 3 4 5 6 7 8 9 10 11 12 13 pgml . train ( project_name TEXT , -- Human-friendly project name task TEXT DEFAULT NULL , -- 'regression' or 'classification' relation_name TEXT DEFAULT NULL , -- name of table or view y_column_name TEXT DEFAULT NULL , -- aka \"label\" or \"unknown\" or \"target\" algorithm TEXT DEFAULT 'linear' , -- statistical learning method hyperparams JSONB DEFAULT '{}' :: JSONB , -- options for the model search TEXT DEFAULT NULL , -- hyperparam tuning, 'grid' or 'random' search_params JSONB DEFAULT '{}' :: JSONB , -- hyperparam search space search_args JSONB DEFAULT '{}' :: JSONB , -- hyperparam options test_size REAL DEFAULT 0 . 25 , -- fraction of the data for the test set test_sampling TEXT DEFAULT 'random' -- 'random', 'first' or 'last' ) Example A minimal first call for a project looks like: SELECT * FROM pgml . train ( 'My Classification Project' , 'classification' , 'my_table_name' , 'my_tables_target_column_name' ); The train function requires an task the first time a project_name is used. That task is either regression or classification , which determines the relevant metrics and analysis performed for models trained toward a common goal. It also requires a relation_name and y_column_name that will be used to establish the first Snapshot of training and test data. By default, 25% of the data (specified by test_size ) will be randomly sampled to measure the performance of the model after the algorithm has been fit to the rest. Tip Postgres supports named arguments for function calls, which allows you to pass only the arguments you need. pgml . train ( 'Project Name' , algorithm => 'xgboost' ) Future calls to train may restate the same task for a project, or omit it, but can't change it. Projects manage their active model using the metrics relevant to a particular task, so changing it would mean some models in the project are no longer directly comparable. In that case, it's better to start a new project. Note If you'd like to train multiple models on the same Snapshot , follow up calls to train may omit the relation_name , y_column_name , test_size and test_sampling arguments to reuse identical data with multiple algorithms or hyperparams. The Snapshot is also saved after training runs for any follow up analysis required. Getting training data \u00b6 A large part of machine learning is acquiring, cleaning and preparing data for algorithms. Naturally, we think Postgres is a great place to store your data. For the purpose of this example, we'll load a toy dataset, a classic handwritten digits image collection from scikit-learn. SQL Output 1 pgml_development =# SELECT pgml . load_dataset ( 'digits' ); 1 2 3 4 5 NOTICE : table \"digits\" does not exist , skipping -- (1) load_dataset -------------- OK ( 1 row ) This NOTICE can safely be ignored. PostgresML attempts to do a clean reload by dropping the pgml.digits table if it exists. The first time this command is run, the table does not exist. PostgresML loads this into a fixed table pgml.digits . You can examine the 2D arrays of image data, as well as the label in the target column. SQL Output 1 pgml_development =# SELECT target , image FROM pgml . digits LIMIT 5 ; 1 2 3 4 5 6 7 8 target | image --------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------- 0 | {{ 0 , 0 , 5 , 13 , 9 , 1 , 0 , 0 } , { 0 , 0 , 13 , 15 , 10 , 15 , 5 , 0 } , { 0 , 3 , 15 , 2 , 0 , 11 , 8 , 0 } , { 0 , 4 , 12 , 0 , 0 , 8 , 8 , 0 } , { 0 , 5 , 8 , 0 , 0 , 9 , 8 , 0 } , { 0 , 4 , 11 , 0 , 1 , 12 , 7 , 0 } , { 0 , 2 , 14 , 5 , 10 , 12 , 0 , 0 } , { 0 , 0 , 6 , 13 , 10 , 0 , 0 , 0 }} 1 | {{ 0 , 0 , 0 , 12 , 13 , 5 , 0 , 0 } , { 0 , 0 , 0 , 11 , 16 , 9 , 0 , 0 } , { 0 , 0 , 3 , 15 , 16 , 6 , 0 , 0 } , { 0 , 7 , 15 , 16 , 16 , 2 , 0 , 0 } , { 0 , 0 , 1 , 16 , 16 , 3 , 0 , 0 } , { 0 , 0 , 1 , 16 , 16 , 6 , 0 , 0 } , { 0 , 0 , 1 , 16 , 16 , 6 , 0 , 0 } , { 0 , 0 , 0 , 11 , 16 , 10 , 0 , 0 }} 2 | {{ 0 , 0 , 0 , 4 , 15 , 12 , 0 , 0 } , { 0 , 0 , 3 , 16 , 15 , 14 , 0 , 0 } , { 0 , 0 , 8 , 13 , 8 , 16 , 0 , 0 } , { 0 , 0 , 1 , 6 , 15 , 11 , 0 , 0 } , { 0 , 1 , 8 , 13 , 15 , 1 , 0 , 0 } , { 0 , 9 , 16 , 16 , 5 , 0 , 0 , 0 } , { 0 , 3 , 13 , 16 , 16 , 11 , 5 , 0 } , { 0 , 0 , 0 , 3 , 11 , 16 , 9 , 0 }} 3 | {{ 0 , 0 , 7 , 15 , 13 , 1 , 0 , 0 } , { 0 , 8 , 13 , 6 , 15 , 4 , 0 , 0 } , { 0 , 2 , 1 , 13 , 13 , 0 , 0 , 0 } , { 0 , 0 , 2 , 15 , 11 , 1 , 0 , 0 } , { 0 , 0 , 0 , 1 , 12 , 12 , 1 , 0 } , { 0 , 0 , 0 , 0 , 1 , 10 , 8 , 0 } , { 0 , 0 , 8 , 4 , 5 , 14 , 9 , 0 } , { 0 , 0 , 7 , 13 , 13 , 9 , 0 , 0 }} 4 | {{ 0 , 0 , 0 , 1 , 11 , 0 , 0 , 0 } , { 0 , 0 , 0 , 7 , 8 , 0 , 0 , 0 } , { 0 , 0 , 1 , 13 , 6 , 2 , 2 , 0 } , { 0 , 0 , 7 , 15 , 0 , 9 , 8 , 0 } , { 0 , 5 , 16 , 10 , 0 , 16 , 6 , 0 } , { 0 , 4 , 15 , 16 , 13 , 16 , 1 , 0 } , { 0 , 0 , 0 , 3 , 15 , 10 , 0 , 0 } , { 0 , 0 , 0 , 2 , 16 , 4 , 0 , 0 }} ( 5 rows ) Training the model \u00b6 Now that we've got data, we're ready to train a model using an algorithm. We'll start with the default linear algorithm to demonstrate the basics. See the Algorithms reference for a complete list of available choices. SQL Output 1 SELECT * FROM pgml . train ( 'Handwritten Digit Image Classifier' , 'classification' , 'pgml.digits' , 'target' ); 1 2 3 4 project_name | task | algorithm_name | status ------------------------------------+----------------+----------------+---------- Handwritten Digit Image Classifier | classification | linear | deployed ( 1 row ) The output gives us information about the training run, including the deployed status. This is great news indicating training has successfully reached a new high score for the project's key metric and our new model was automatically deployed as the one that will be used to make new predictions for the project. See Deployments for a guide to managing the active model. Inspecting the results \u00b6 Now we can inspect some of the artifacts a training run creates. SQL Output 1 SELECT * FROM pgml . overview ; 1 2 3 4 name | deployed_at | task | algorithm_name | relation_name | y_column_name | test_sampling | test_size ------------------------------------+----------------------------+----------------+----------------+---------------+---------------+---------------+----------- Handwritten Digit Image Classifier | 2022 - 05 - 10 15 : 06 : 32 . 824305 | classification | linear | pgml . digits | { target } | random | 0 . 25 ( 1 row ) See the Examples for more kinds of training with different types of features, algorithms and tasks. See the Models reference for a complete description of the artifacts.","title":"Training Overview"},{"location":"user_guides/training/overview/#training","text":"The training function is at the heart of PostgresML. It's a powerful single call that can handle the different tasks of training depending on the arguments passed.","title":"Training"},{"location":"user_guides/training/overview/#api","text":"Most parameters are optional other than the project_name which is a simple human readable identifier to organize your work. pgml.train 1 2 3 4 5 6 7 8 9 10 11 12 13 pgml . train ( project_name TEXT , -- Human-friendly project name task TEXT DEFAULT NULL , -- 'regression' or 'classification' relation_name TEXT DEFAULT NULL , -- name of table or view y_column_name TEXT DEFAULT NULL , -- aka \"label\" or \"unknown\" or \"target\" algorithm TEXT DEFAULT 'linear' , -- statistical learning method hyperparams JSONB DEFAULT '{}' :: JSONB , -- options for the model search TEXT DEFAULT NULL , -- hyperparam tuning, 'grid' or 'random' search_params JSONB DEFAULT '{}' :: JSONB , -- hyperparam search space search_args JSONB DEFAULT '{}' :: JSONB , -- hyperparam options test_size REAL DEFAULT 0 . 25 , -- fraction of the data for the test set test_sampling TEXT DEFAULT 'random' -- 'random', 'first' or 'last' ) Example A minimal first call for a project looks like: SELECT * FROM pgml . train ( 'My Classification Project' , 'classification' , 'my_table_name' , 'my_tables_target_column_name' ); The train function requires an task the first time a project_name is used. That task is either regression or classification , which determines the relevant metrics and analysis performed for models trained toward a common goal. It also requires a relation_name and y_column_name that will be used to establish the first Snapshot of training and test data. By default, 25% of the data (specified by test_size ) will be randomly sampled to measure the performance of the model after the algorithm has been fit to the rest. Tip Postgres supports named arguments for function calls, which allows you to pass only the arguments you need. pgml . train ( 'Project Name' , algorithm => 'xgboost' ) Future calls to train may restate the same task for a project, or omit it, but can't change it. Projects manage their active model using the metrics relevant to a particular task, so changing it would mean some models in the project are no longer directly comparable. In that case, it's better to start a new project. Note If you'd like to train multiple models on the same Snapshot , follow up calls to train may omit the relation_name , y_column_name , test_size and test_sampling arguments to reuse identical data with multiple algorithms or hyperparams. The Snapshot is also saved after training runs for any follow up analysis required.","title":"API"},{"location":"user_guides/training/overview/#getting-training-data","text":"A large part of machine learning is acquiring, cleaning and preparing data for algorithms. Naturally, we think Postgres is a great place to store your data. For the purpose of this example, we'll load a toy dataset, a classic handwritten digits image collection from scikit-learn. SQL Output 1 pgml_development =# SELECT pgml . load_dataset ( 'digits' ); 1 2 3 4 5 NOTICE : table \"digits\" does not exist , skipping -- (1) load_dataset -------------- OK ( 1 row ) This NOTICE can safely be ignored. PostgresML attempts to do a clean reload by dropping the pgml.digits table if it exists. The first time this command is run, the table does not exist. PostgresML loads this into a fixed table pgml.digits . You can examine the 2D arrays of image data, as well as the label in the target column. SQL Output 1 pgml_development =# SELECT target , image FROM pgml . digits LIMIT 5 ; 1 2 3 4 5 6 7 8 target | image --------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------- 0 | {{ 0 , 0 , 5 , 13 , 9 , 1 , 0 , 0 } , { 0 , 0 , 13 , 15 , 10 , 15 , 5 , 0 } , { 0 , 3 , 15 , 2 , 0 , 11 , 8 , 0 } , { 0 , 4 , 12 , 0 , 0 , 8 , 8 , 0 } , { 0 , 5 , 8 , 0 , 0 , 9 , 8 , 0 } , { 0 , 4 , 11 , 0 , 1 , 12 , 7 , 0 } , { 0 , 2 , 14 , 5 , 10 , 12 , 0 , 0 } , { 0 , 0 , 6 , 13 , 10 , 0 , 0 , 0 }} 1 | {{ 0 , 0 , 0 , 12 , 13 , 5 , 0 , 0 } , { 0 , 0 , 0 , 11 , 16 , 9 , 0 , 0 } , { 0 , 0 , 3 , 15 , 16 , 6 , 0 , 0 } , { 0 , 7 , 15 , 16 , 16 , 2 , 0 , 0 } , { 0 , 0 , 1 , 16 , 16 , 3 , 0 , 0 } , { 0 , 0 , 1 , 16 , 16 , 6 , 0 , 0 } , { 0 , 0 , 1 , 16 , 16 , 6 , 0 , 0 } , { 0 , 0 , 0 , 11 , 16 , 10 , 0 , 0 }} 2 | {{ 0 , 0 , 0 , 4 , 15 , 12 , 0 , 0 } , { 0 , 0 , 3 , 16 , 15 , 14 , 0 , 0 } , { 0 , 0 , 8 , 13 , 8 , 16 , 0 , 0 } , { 0 , 0 , 1 , 6 , 15 , 11 , 0 , 0 } , { 0 , 1 , 8 , 13 , 15 , 1 , 0 , 0 } , { 0 , 9 , 16 , 16 , 5 , 0 , 0 , 0 } , { 0 , 3 , 13 , 16 , 16 , 11 , 5 , 0 } , { 0 , 0 , 0 , 3 , 11 , 16 , 9 , 0 }} 3 | {{ 0 , 0 , 7 , 15 , 13 , 1 , 0 , 0 } , { 0 , 8 , 13 , 6 , 15 , 4 , 0 , 0 } , { 0 , 2 , 1 , 13 , 13 , 0 , 0 , 0 } , { 0 , 0 , 2 , 15 , 11 , 1 , 0 , 0 } , { 0 , 0 , 0 , 1 , 12 , 12 , 1 , 0 } , { 0 , 0 , 0 , 0 , 1 , 10 , 8 , 0 } , { 0 , 0 , 8 , 4 , 5 , 14 , 9 , 0 } , { 0 , 0 , 7 , 13 , 13 , 9 , 0 , 0 }} 4 | {{ 0 , 0 , 0 , 1 , 11 , 0 , 0 , 0 } , { 0 , 0 , 0 , 7 , 8 , 0 , 0 , 0 } , { 0 , 0 , 1 , 13 , 6 , 2 , 2 , 0 } , { 0 , 0 , 7 , 15 , 0 , 9 , 8 , 0 } , { 0 , 5 , 16 , 10 , 0 , 16 , 6 , 0 } , { 0 , 4 , 15 , 16 , 13 , 16 , 1 , 0 } , { 0 , 0 , 0 , 3 , 15 , 10 , 0 , 0 } , { 0 , 0 , 0 , 2 , 16 , 4 , 0 , 0 }} ( 5 rows )","title":"Getting training data"},{"location":"user_guides/training/overview/#training-the-model","text":"Now that we've got data, we're ready to train a model using an algorithm. We'll start with the default linear algorithm to demonstrate the basics. See the Algorithms reference for a complete list of available choices. SQL Output 1 SELECT * FROM pgml . train ( 'Handwritten Digit Image Classifier' , 'classification' , 'pgml.digits' , 'target' ); 1 2 3 4 project_name | task | algorithm_name | status ------------------------------------+----------------+----------------+---------- Handwritten Digit Image Classifier | classification | linear | deployed ( 1 row ) The output gives us information about the training run, including the deployed status. This is great news indicating training has successfully reached a new high score for the project's key metric and our new model was automatically deployed as the one that will be used to make new predictions for the project. See Deployments for a guide to managing the active model.","title":"Training the model"},{"location":"user_guides/training/overview/#inspecting-the-results","text":"Now we can inspect some of the artifacts a training run creates. SQL Output 1 SELECT * FROM pgml . overview ; 1 2 3 4 name | deployed_at | task | algorithm_name | relation_name | y_column_name | test_sampling | test_size ------------------------------------+----------------------------+----------------+----------------+---------------+---------------+---------------+----------- Handwritten Digit Image Classifier | 2022 - 05 - 10 15 : 06 : 32 . 824305 | classification | linear | pgml . digits | { target } | random | 0 . 25 ( 1 row ) See the Examples for more kinds of training with different types of features, algorithms and tasks. See the Models reference for a complete description of the artifacts.","title":"Inspecting the results"},{"location":"user_guides/transformers/fine_tuning/","text":"Fine Tuning \u00b6 Pre-trained models allow you to get up and running quickly, but you can likely improve performance on your dataset by fine tuning them. Normally, you'll bring your own data to the party, but for these examples we'll use datasets published on Hugging Face. Make sure you've installed the required data dependencies detailed in setup . Translation Example \u00b6 The Helsinki-NLP organization provides more than a thousand pre-trained models to translate between different language pairs. These can be further fine tuned on additional datasets with domain specific vocabulary. Researchers have also created large collections of documents that have been manually translated across languages by experts for training data. Prepare the data \u00b6 The kde4 dataset contains many language pairs. Subsets can be loaded into your Postgres instance with a call to pgml.load_dataset , or you may wish to create your own fine tuning dataset with vocabulary specific to your domain. load_data.sql 1 SELECT pgml . load_dataset ( 'kde4' , kwargs => '{\"lang1\": \"en\", \"lang2\": \"es\"}' ); You can view the newly loaded data in your Postgres database: SQL Result 1 SELECT * FROM pgml . kde4 LIMIT 5 ; 1 2 3 4 5 6 7 8 9 10 id | translation -----+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- 99 | { \"en\" : \"If you wish to manipulate the DOM tree in any way you will have to use an external script to do so.\" , \"es\" : \"Si desea manipular el \u00e1rbol DOM deber\u00e1 utilizar un script externo para hacerlo.\" } 100 | { \"en\" : \"Credits\" , \"es\" : \"Cr\u00e9ditos\" } 101 | { \"en\" : \"The domtreeviewer plugin is Copyright & copy; 2001 The Kafka Team/ Andreas Schlapbach kde-kafka@master. kde. org schlpbch@unibe. ch\" , \"es\" : \"Derechos de autor de la extensi\u00f3n domtreeviewer & copy;. 2001. El equipo de Kafka/ Andreas Schlapbach kde-kafka@master. kde. org schlpbch@unibe. ch.\" } 102 | { \"en\" : \"Josef Weidendorfer Josef. Weidendorfer@gmx. de\" , \"es\" : \"Josef Weidendorfer Josef. Weidendorfer@gmx. de\" } 103 | { \"en\" : \"ROLES_OF_TRANSLATORS\" , \"es\" : \"Rafael Osuna rosuna@wol. es Traductor\" } ( 5 rows ) When you're constructing your own datasets for translation, it's important to mirror the same table structure. You'll need a JSONB column named translation , that has first has a \"from\" language name/value pair, and then a \"to\" language name/value pair. In this English to Spanish example we use from \"en\" to \"es\". You'll pass a y_column_name of translation to tune the model. Tune the model \u00b6 Tuning is very similar to training with PostgresML, although we specify a model_name to download from Hugging Face instead of the base algorithm . tune.sql 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 SELECT pgml . tune ( 'Translate English to Spanish' , task => 'translation_en_to_es' , relation_name => 'pgml.kde4' , y_column_name => 'translation' , model_name => 'Helsinki-NLP/opus-mt-en-es' , hyperparams => '{ \"learning_rate\": 2e-5, \"per_device_train_batch_size\": 16, \"per_device_eval_batch_size\": 16, \"num_train_epochs\": 1, \"weight_decay\": 0.01, \"max_length\": 128 }' , test_size => 0 . 5 , test_sampling => 'last' ); Generate Translations \u00b6 Note Translations use the pgml.generate API since they return TEXT rather than numeric values. You may also call pgml.generate with a TEXT[] for batch processing. SQL Result 1 2 SELECT pgml . generate ( 'Translate English to Spanish' , 'I love SQL' ) AS spanish ; 1 2 3 4 5 6 spanish ---------------- Me encanta SQL ( 1 row ) Time : 126 . 837 ms See the task documentation for more examples, use cases, models and datasets. Text Classification Example \u00b6 DistilBERT is a small, fast, cheap and light Transformer model based on the BERT architecture. It can be fine tuned on specific datasets to learn further nuance between positive and negative examples. For this example, we'll fine tune distilbert-base-uncased on the IMBD dataset, which is a list of movie reviews along with a positive or negative label. Without tuning, DistilBERT classifies every single movie review as positive , and has a F 1 score of 0.367, which is about what you'd expect for a relatively useless classifier. However, after training for a single epoch (takes about 10 minutes on an Nvidia 1080 TI), the F 1 jumps to 0.928 which is a huge improvement, indicating DistilBERT can now fairly accurately predict sentiment from IMDB reviews. Further training for another epoch only results in a very minor improvement to 0.931, and the 3 rd epoch is flat, also at 0.931 which indicates DistilBERT is unlikely to continue learning more about this particular dataset with additional training. You can view the results of each model, like those trained from scratch, in the dashboard. Once our model has been fine tuned on the dataset, it'll be saved and deployed with a Project visible in the Dashboard, just like models built from simpler algorithms. Prepare the data \u00b6 The IMDB dataset has 50,000 examples of user reviews with positive or negative viewing experiences as the labels, and is split 50/50 into training and evaluation datasets. load_dataset.sql 1 SELECT pgml . load_dataset ( 'imdb' ); You can view the newly loaded data in your Postgres database: SQL Result 1 SELECT * FROM pgml . imdb LIMIT 1 ; 1 2 3 4 text | label -----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------- This has to be the funniest stand up comedy I have ever seen . Eddie Izzard is a genius , he picks in Brits , Americans and everyone in between . His style is completely natural and completely hilarious . I doubt that anyone could sit through this and not laugh their a ** off . Watch , enjoy , it ' s funny . | 1 ( 1 row ) Tune the model \u00b6 Tuning has a nearly identical API to training, except you may pass the name of a model published on Hugging Face to start with, rather than training an algorithm from scratch. tune.sql 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 SELECT pgml . tune ( 'IMDB Review Sentiment' , task => 'text-classification' , relation_name => 'pgml.imdb' , y_column_name => 'label' , model_name => 'distilbert-base-uncased' , hyperparams => '{ \"learning_rate\": 2e-5, \"per_device_train_batch_size\": 16, \"per_device_eval_batch_size\": 16, \"num_train_epochs\": 1, \"weight_decay\": 0.01 }' , test_size => 0 . 5 , test_sampling => 'last' ); Make predictions \u00b6 SQL Result 1 2 SELECT pgml . predict ( 'IMDB Review Sentiment' , 'I love SQL' ) AS sentiment ; 1 2 3 4 5 6 sentiment ----------- 1 ( 1 row ) Time : 16 . 681 ms The default for predict in a classification problem classifies the statement as one of the labels. In this case, 0 is negative and 1 is positive. If you'd like to check the individual probabilities associated with each class you can use the predict_proba API: SQL Result 1 2 SELECT pgml . predict_proba ( 'IMDB Review Sentiment' , 'I love SQL' ) AS sentiment ; 1 2 3 4 5 6 sentiment ------------------------------------------- [ 0 . 06266672909259796 , 0 . 9373332858085632 ] ( 1 row ) Time : 18 . 101 ms This shows that there is a 6.26% chance for category 0 (negative sentiment), and a 93.73% chance it's category 1 (postive sentiment). See the task documentation for more examples, use cases, models and datasets. Summarization Example \u00b6 At a high level, summarization uses similar techniques to translation. Both use an input sequence to generate an output sequence. The difference being that summarization extracts the most relevant parts of the input sequence to generate the output. Prepare the data \u00b6 BillSum is a dataset with training examples that summarize US Congressional and California state bills. You can pass kwargs specific to loading datasets, in this case we'll restrict the dataset to California samples: load_dataset.sql 1 SELECT pgml . load_dataset ( 'billsum' , kwargs => '{\"split\": \"ca_test\"}' ); You can view the newly loaded data in your Postgres database: SQL Result 1 SELECT * FROM pgml . billsum LIMIT 1 ; 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 text | summary | title -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------- The people of the State of California do enact as follows : +| Existing property tax law establishes a veterans \u2019 organization exemption under which property is exempt from taxation if , among other things , that property is used exclusively for charitable purposes and is owned by a veterans \u2019 organization . +| An act to amend Section 215 . 1 of the Revenue and Taxation Code , relating to taxation , to take effect immediately , tax levy . +| This bill would provide that the veterans \u2019 organization exemption shall not be denied to a property on the basis that the property is used for fraternal , lodge , or social club purposes , and would make specific findings and declarations in that regard . The bill would also provide that the exemption shall not apply to any portion of a property that consists of a bar where alcoholic beverages are served . +| +| Section 2229 of the Revenue and Taxation Code requires the Legislature to reimburse local agencies annually for certain property tax revenues lost as a result of any exemption or classification of property for purposes of ad valorem property taxation . +| SECTION 1 . +| This bill would provide that , notwithstanding Section 2229 of the Revenue and Taxation Code , no appropriation is made and the state shall not reimburse local agencies for property tax revenues lost by them pursuant to the bill . +| The Legislature finds and declares all of the following : +| This bill would take effect immediately as a tax levy . | ( a ) ( 1 ) Since 1899 congressionally chartered veterans \u2019 organizations have provided a valuable service to our nation \u2019 s returning service members . These organizations help preserve the memories and incidents of the great hostilities fought by our nation , and preserve and strengthen comradeship among members . +| | ( 2 ) These veterans \u2019 organizations also own and manage various properties including lodges , posts , and fraternal halls . These properties act as a safe haven where veterans of all ages and their families can gather together to find camaraderie and fellowship , share stories , and seek support from people who understand their unique experiences . This aids in the healing process for these returning veterans , and ensures their health and happiness . +| | ( b ) As a result of congressional chartering of these veterans \u2019 organizations , the United States Internal Revenue Service created a special tax exemption for these organizations under Section 501 ( c )( 19 ) of the Internal Revenue Code . +| | ( c ) Section 501 ( c )( 19 ) of the Internal Revenue Code and related federal regulations provide for the exemption for posts or organizations of war veterans , or an auxiliary unit or society of , or a trust or foundation for , any such post or organization that , among other attributes , carries on programs to perpetuate the memory of deceased veterans and members of the Armed Forces and to comfort their survivors , conducts programs for religious , charitable , scientific , literary , or educational purposes , sponsors or participates in activities of a patriotic nature , and provides social and recreational activities for their members . +| | ( d ) Section 215 . 1 of the Revenue and Taxation Code stipulates that all buildings , support and so much of the real property on which the buildings are situated as may be required for the convenient use and occupation of the buildings , used exclusively for charitable purposes , owned by a veterans \u2019 organization that has been chartered by the Congress of the United States , organized and operated for charitable purposes , when the same are used solely and exclusively for the purpose of the organization , if not conducted for profit and no part of the net earnings of which ensures to the benefit of any private individual or member thereof , are exempt from taxation . +| | ( e ) The Chief Counsel of the State Board of Equalization concluded , based on a 1979 appellate court decision , that only parts of American Legion halls are exempt from property taxation and that other parts , such as billiard rooms , card rooms , and similar areas , are not exempt . +| | ( f ) In a 1994 memorandum , the State Board of Equalization \u2019 s legal division further concluded that the areas normally considered eligible for exemptions are the office areas used to counsel veterans and the area used to store veterans \u2019 records , but that the meeting hall and bar found in most of the facilities are not considered used for charitable purposes . +| | ( g ) Tax - exempt status is intended to provide economic incentive and support to veterans \u2019 organizations to provide for the social welfare of the community of current and former military personnel . +| | ( h ) The State Board of Equalization \u2019 s constriction of the tax exemption has resulted in an onerous tax burden on California veteran service organizations posts or halls , hinders the posts \u2019 ability to provide facilities for veterans , and threatens the economic viability of many local organizations . +| | ( i ) The charitable activities of a veteran service organizations post or hall are much more than the counseling of veterans . The requirements listed for qualification for the federal tax exemption clearly dictate a need for more than just an office . +| | ( j ) Programs to perpetuate the memory of deceased veterans and members of the Armed Forces and to comfort their survivors require the use of facilities for funerals and receptions . +| | ( k ) Programs for religious , charitable , scientific , literary , or educational purposes require space for more than 50 attendees . +| | ( l ) Activities of a patriotic nature need facilities to accommodate hundreds of people . +| | ( m ) Social and recreational activities for members require precisely those areas considered \u201c not used for charitable purposes \u201d by the State Board of Equalization . +| | ( n ) The State Board of Equalization \u2019 s interpretation of the Revenue and Taxation Code reflects a lack of understanding of the purpose and programs of the veterans service organizations posts or halls and is detrimental to the good works performed in support of our veteran community . +| +| ( g ) Tax - exempt status is intended to provide economic incentive and support to veterans \u2019 organizations to provide for the social welfare of the community of current and former military personnel . +| | ( h ) The State Board of Equalization \u2019 s constriction of the tax exemption has resulted in an onerous tax burden on California veteran service organizations posts or halls , hinders the posts \u2019 ability to provide facilities for veterans , and threatens the economic viability of many local organizations . +| | ( i ) The charitable activities of a veteran service organizations post or hall are much more than the counseling of veterans . The requirements listed for qualification for the federal tax exemption clearly dictate a need for more than just an office . +| | ( j ) Programs to perpetuate the memory of deceased veterans and members of the Armed Forces and to comfort their survivors require the use of facilities for funerals and receptions . +| | ( k ) Programs for religious , charitable , scientific , literary , or educational purposes require space for more than 50 attendees . +| | ( l ) Activities of a patriotic nature need facilities to accommodate hundreds of people . +| | ( m ) Social and recreational activities for members require precisely those areas considered \u201c not used for charitable purposes \u201d by the State Board of Equalization . +| | ( n ) The State Board of Equalization \u2019 s interpretation of the Revenue and Taxation Code reflects a lack of understanding of the purpose and programs of the veterans service organizations posts or halls and is detrimental to the good works performed in support of our veteran community . +| | SECTION 1 . +| | SEC . 2 . +| | Section 215 . 1 of the Revenue and Taxation Code is amended to read : +| | 215 . 1 . +| | ( a ) All buildings , and so much of the real property on which the buildings are situated as may be required for the convenient use and occupation of the buildings , used exclusively for charitable purposes , owned by a veterans \u2019 organization that has been chartered by the Congress of the United States , organized and operated for charitable purposes , and exempt from federal income tax as an organization described in Section 501 ( c )( 19 ) of the Internal Revenue Code when the same are used solely and exclusively for the purpose of the organization , if not conducted for profit and no part of the net earnings of which inures to the benefit of any private individual or member thereof , shall be exempt from taxation . +| | ( b ) The exemption provided for in this section shall apply to the property of all organizations meeting the requirements of this section , subdivision ( b ) of Section 4 of Article XIII of the California Constitution , and paragraphs ( 1 ) to ( 4 ), inclusive , ( 6 ), and ( 7 ) of subdivision ( a ) of Section 214 . +| | ( c ) ( 1 ) The exemption specified by subdivision ( a ) shall not be denied to a property on the basis that the property is used for fraternal , lodge , or social club purposes . +| | ( 2 ) With regard to this subdivision , the Legislature finds and declares all of the following : +| | ( A ) The exempt activities of a veterans \u2019 organization as described in subdivision ( a ) qualitatively differ from the exempt activities of other nonprofit entities that use property for fraternal , lodge , or social club purposes in that the exempt purpose of the veterans \u2019 organization is to conduct programs to perpetuate the memory of deceased veterans and members of the Armed Forces and to comfort their survivors , to conduct programs for religious , charitable , scientific , literary , or educational purposes , to sponsor or participate in activities of a patriotic nature , and to provide social and recreational activities for their members . +| | ( B ) In light of this distinction , the use of real property by a veterans \u2019 organization as described in subdivision ( a ), for fraternal , lodge , or social club purposes is central to that organization \u2019 s exempt purposes and activities . +| | ( C ) In light of the factors set forth in subparagraphs ( A ) and ( B ), the use of real property by a veterans \u2019 organization as described in subdivision ( a ) for fraternal , lodge , or social club purposes , constitutes the exclusive use of that property for a charitable purpose within the meaning of subdivision ( b ) of Section 4 of Article XIII of the California Constitution . +| | ( d ) The exemption provided for in this section shall not apply to any portion of a property that consists of a bar where alcoholic beverages are served . The portion of the property ineligible for the veterans \u2019 organization exemption shall be that area used primarily to prepare and serve alcoholic beverages . +| | ( e ) An organization that files a claim for the exemption provided for in this section shall file with the assessor a valid organizational clearance certificate issued pursuant to Section 254 . 6 . +| | ( f ) This exemption shall be known as the \u201c veterans \u2019 organization exemption . \u201d +| | SEC . 2 . +| | SEC . 3 . +| | Notwithstanding Section 2229 of the Revenue and Taxation Code , no appropriation is made by this act and the state shall not reimburse any local agency for any property tax revenues lost by it pursuant to this act . +| | SEC . 3 . +| | SEC . 4 . +| | This act provides for a tax levy within the meaning of Article IV of the Constitution and shall go into immediate effect . | | ( 1 row ) This dataset has 3 fields, but summarization transformers only take a single input to produce their output. We can create a view that simply omits the title from the training data: omit_title.sql 1 2 CREATE OR REPLACE VIEW billsum_training_data AS SELECT \"text\" , summary FROM pgml . billsum ; Or, it might be interesting to concat the title to the text field to see how relevant it actually is to the bill. If the title of a bill is the first sentence, and doesn't appear in summary, it may indicate that it's a poorly chosen title for the bill: concat_title.sql 1 2 CREATE OR REPLACE VIEW billsum_training_data AS SELECT title || '\\n' || \"text\" AS \"text\" , summary FROM pgml . billsum ; Tune the model \u00b6 Tuning has a nearly identical API to training, except you may pass the name of a model published on Hugging Face to start with, rather than training an algorithm from scratch. tune.sql 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 SELECT pgml . tune ( 'Legal Summarization' , task => 'summarization' , relation_name => 'billsum_training_data' , y_column_name => 'summary' , model_name => 'sshleifer/distilbart-xsum-12-1' , hyperparams => '{ \"learning_rate\": 2e-5, \"per_device_train_batch_size\": 2, \"per_device_eval_batch_size\": 2, \"num_train_epochs\": 1, \"weight_decay\": 0.01, \"max_input_length\": 1024, \"max_summary_length\": 128 }' , test_size => 0 . 2 , test_sampling => 'last' ); Make predictions \u00b6 SQL Result 1 SELECT pgml . predict ( 'IMDB Review Sentiment' , 'I love SQL' ) AS sentiment ; ```sql linenumes=\"1\" sentiment 1 (1 row) Time: 16.681 ms ``` The default for predict in a classification problem classifies the statement as one of the labels. In this case 0 is negative and 1 is positive. If you'd like to check the individual probabilities associated with each class you can use the predict_proba API SQL Result 1 SELECT pgml . predict_proba ( 'IMDB Review Sentiment' , 'I love SQL' ) AS sentiment ; ```sql linenumes=\"1\" sentiment [0.06266672909259796, 0.9373332858085632] (1 row) Time: 18.101 ms ``` This shows that there is a 6.26% chance for category 0 (negative sentiment), and a 93.73% chance it's category 1 (postive sentiment). See the task documentation for more examples, use cases, models and datasets.","title":"Fine Tuning"},{"location":"user_guides/transformers/fine_tuning/#fine-tuning","text":"Pre-trained models allow you to get up and running quickly, but you can likely improve performance on your dataset by fine tuning them. Normally, you'll bring your own data to the party, but for these examples we'll use datasets published on Hugging Face. Make sure you've installed the required data dependencies detailed in setup .","title":"Fine Tuning"},{"location":"user_guides/transformers/fine_tuning/#translation-example","text":"The Helsinki-NLP organization provides more than a thousand pre-trained models to translate between different language pairs. These can be further fine tuned on additional datasets with domain specific vocabulary. Researchers have also created large collections of documents that have been manually translated across languages by experts for training data.","title":"Translation Example"},{"location":"user_guides/transformers/fine_tuning/#prepare-the-data","text":"The kde4 dataset contains many language pairs. Subsets can be loaded into your Postgres instance with a call to pgml.load_dataset , or you may wish to create your own fine tuning dataset with vocabulary specific to your domain. load_data.sql 1 SELECT pgml . load_dataset ( 'kde4' , kwargs => '{\"lang1\": \"en\", \"lang2\": \"es\"}' ); You can view the newly loaded data in your Postgres database: SQL Result 1 SELECT * FROM pgml . kde4 LIMIT 5 ; 1 2 3 4 5 6 7 8 9 10 id | translation -----+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- 99 | { \"en\" : \"If you wish to manipulate the DOM tree in any way you will have to use an external script to do so.\" , \"es\" : \"Si desea manipular el \u00e1rbol DOM deber\u00e1 utilizar un script externo para hacerlo.\" } 100 | { \"en\" : \"Credits\" , \"es\" : \"Cr\u00e9ditos\" } 101 | { \"en\" : \"The domtreeviewer plugin is Copyright & copy; 2001 The Kafka Team/ Andreas Schlapbach kde-kafka@master. kde. org schlpbch@unibe. ch\" , \"es\" : \"Derechos de autor de la extensi\u00f3n domtreeviewer & copy;. 2001. El equipo de Kafka/ Andreas Schlapbach kde-kafka@master. kde. org schlpbch@unibe. ch.\" } 102 | { \"en\" : \"Josef Weidendorfer Josef. Weidendorfer@gmx. de\" , \"es\" : \"Josef Weidendorfer Josef. Weidendorfer@gmx. de\" } 103 | { \"en\" : \"ROLES_OF_TRANSLATORS\" , \"es\" : \"Rafael Osuna rosuna@wol. es Traductor\" } ( 5 rows ) When you're constructing your own datasets for translation, it's important to mirror the same table structure. You'll need a JSONB column named translation , that has first has a \"from\" language name/value pair, and then a \"to\" language name/value pair. In this English to Spanish example we use from \"en\" to \"es\". You'll pass a y_column_name of translation to tune the model.","title":"Prepare the data"},{"location":"user_guides/transformers/fine_tuning/#tune-the-model","text":"Tuning is very similar to training with PostgresML, although we specify a model_name to download from Hugging Face instead of the base algorithm . tune.sql 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 SELECT pgml . tune ( 'Translate English to Spanish' , task => 'translation_en_to_es' , relation_name => 'pgml.kde4' , y_column_name => 'translation' , model_name => 'Helsinki-NLP/opus-mt-en-es' , hyperparams => '{ \"learning_rate\": 2e-5, \"per_device_train_batch_size\": 16, \"per_device_eval_batch_size\": 16, \"num_train_epochs\": 1, \"weight_decay\": 0.01, \"max_length\": 128 }' , test_size => 0 . 5 , test_sampling => 'last' );","title":"Tune the model"},{"location":"user_guides/transformers/fine_tuning/#generate-translations","text":"Note Translations use the pgml.generate API since they return TEXT rather than numeric values. You may also call pgml.generate with a TEXT[] for batch processing. SQL Result 1 2 SELECT pgml . generate ( 'Translate English to Spanish' , 'I love SQL' ) AS spanish ; 1 2 3 4 5 6 spanish ---------------- Me encanta SQL ( 1 row ) Time : 126 . 837 ms See the task documentation for more examples, use cases, models and datasets.","title":"Generate Translations"},{"location":"user_guides/transformers/fine_tuning/#text-classification-example","text":"DistilBERT is a small, fast, cheap and light Transformer model based on the BERT architecture. It can be fine tuned on specific datasets to learn further nuance between positive and negative examples. For this example, we'll fine tune distilbert-base-uncased on the IMBD dataset, which is a list of movie reviews along with a positive or negative label. Without tuning, DistilBERT classifies every single movie review as positive , and has a F 1 score of 0.367, which is about what you'd expect for a relatively useless classifier. However, after training for a single epoch (takes about 10 minutes on an Nvidia 1080 TI), the F 1 jumps to 0.928 which is a huge improvement, indicating DistilBERT can now fairly accurately predict sentiment from IMDB reviews. Further training for another epoch only results in a very minor improvement to 0.931, and the 3 rd epoch is flat, also at 0.931 which indicates DistilBERT is unlikely to continue learning more about this particular dataset with additional training. You can view the results of each model, like those trained from scratch, in the dashboard. Once our model has been fine tuned on the dataset, it'll be saved and deployed with a Project visible in the Dashboard, just like models built from simpler algorithms.","title":"Text Classification Example"},{"location":"user_guides/transformers/fine_tuning/#prepare-the-data_1","text":"The IMDB dataset has 50,000 examples of user reviews with positive or negative viewing experiences as the labels, and is split 50/50 into training and evaluation datasets. load_dataset.sql 1 SELECT pgml . load_dataset ( 'imdb' ); You can view the newly loaded data in your Postgres database: SQL Result 1 SELECT * FROM pgml . imdb LIMIT 1 ; 1 2 3 4 text | label -----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------- This has to be the funniest stand up comedy I have ever seen . Eddie Izzard is a genius , he picks in Brits , Americans and everyone in between . His style is completely natural and completely hilarious . I doubt that anyone could sit through this and not laugh their a ** off . Watch , enjoy , it ' s funny . | 1 ( 1 row )","title":"Prepare the data"},{"location":"user_guides/transformers/fine_tuning/#tune-the-model_1","text":"Tuning has a nearly identical API to training, except you may pass the name of a model published on Hugging Face to start with, rather than training an algorithm from scratch. tune.sql 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 SELECT pgml . tune ( 'IMDB Review Sentiment' , task => 'text-classification' , relation_name => 'pgml.imdb' , y_column_name => 'label' , model_name => 'distilbert-base-uncased' , hyperparams => '{ \"learning_rate\": 2e-5, \"per_device_train_batch_size\": 16, \"per_device_eval_batch_size\": 16, \"num_train_epochs\": 1, \"weight_decay\": 0.01 }' , test_size => 0 . 5 , test_sampling => 'last' );","title":"Tune the model"},{"location":"user_guides/transformers/fine_tuning/#make-predictions","text":"SQL Result 1 2 SELECT pgml . predict ( 'IMDB Review Sentiment' , 'I love SQL' ) AS sentiment ; 1 2 3 4 5 6 sentiment ----------- 1 ( 1 row ) Time : 16 . 681 ms The default for predict in a classification problem classifies the statement as one of the labels. In this case, 0 is negative and 1 is positive. If you'd like to check the individual probabilities associated with each class you can use the predict_proba API: SQL Result 1 2 SELECT pgml . predict_proba ( 'IMDB Review Sentiment' , 'I love SQL' ) AS sentiment ; 1 2 3 4 5 6 sentiment ------------------------------------------- [ 0 . 06266672909259796 , 0 . 9373332858085632 ] ( 1 row ) Time : 18 . 101 ms This shows that there is a 6.26% chance for category 0 (negative sentiment), and a 93.73% chance it's category 1 (postive sentiment). See the task documentation for more examples, use cases, models and datasets.","title":"Make predictions"},{"location":"user_guides/transformers/fine_tuning/#summarization-example","text":"At a high level, summarization uses similar techniques to translation. Both use an input sequence to generate an output sequence. The difference being that summarization extracts the most relevant parts of the input sequence to generate the output.","title":"Summarization Example"},{"location":"user_guides/transformers/fine_tuning/#prepare-the-data_2","text":"BillSum is a dataset with training examples that summarize US Congressional and California state bills. You can pass kwargs specific to loading datasets, in this case we'll restrict the dataset to California samples: load_dataset.sql 1 SELECT pgml . load_dataset ( 'billsum' , kwargs => '{\"split\": \"ca_test\"}' ); You can view the newly loaded data in your Postgres database: SQL Result 1 SELECT * FROM pgml . billsum LIMIT 1 ; 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 text | summary | title -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------- The people of the State of California do enact as follows : +| Existing property tax law establishes a veterans \u2019 organization exemption under which property is exempt from taxation if , among other things , that property is used exclusively for charitable purposes and is owned by a veterans \u2019 organization . +| An act to amend Section 215 . 1 of the Revenue and Taxation Code , relating to taxation , to take effect immediately , tax levy . +| This bill would provide that the veterans \u2019 organization exemption shall not be denied to a property on the basis that the property is used for fraternal , lodge , or social club purposes , and would make specific findings and declarations in that regard . The bill would also provide that the exemption shall not apply to any portion of a property that consists of a bar where alcoholic beverages are served . +| +| Section 2229 of the Revenue and Taxation Code requires the Legislature to reimburse local agencies annually for certain property tax revenues lost as a result of any exemption or classification of property for purposes of ad valorem property taxation . +| SECTION 1 . +| This bill would provide that , notwithstanding Section 2229 of the Revenue and Taxation Code , no appropriation is made and the state shall not reimburse local agencies for property tax revenues lost by them pursuant to the bill . +| The Legislature finds and declares all of the following : +| This bill would take effect immediately as a tax levy . | ( a ) ( 1 ) Since 1899 congressionally chartered veterans \u2019 organizations have provided a valuable service to our nation \u2019 s returning service members . These organizations help preserve the memories and incidents of the great hostilities fought by our nation , and preserve and strengthen comradeship among members . +| | ( 2 ) These veterans \u2019 organizations also own and manage various properties including lodges , posts , and fraternal halls . These properties act as a safe haven where veterans of all ages and their families can gather together to find camaraderie and fellowship , share stories , and seek support from people who understand their unique experiences . This aids in the healing process for these returning veterans , and ensures their health and happiness . +| | ( b ) As a result of congressional chartering of these veterans \u2019 organizations , the United States Internal Revenue Service created a special tax exemption for these organizations under Section 501 ( c )( 19 ) of the Internal Revenue Code . +| | ( c ) Section 501 ( c )( 19 ) of the Internal Revenue Code and related federal regulations provide for the exemption for posts or organizations of war veterans , or an auxiliary unit or society of , or a trust or foundation for , any such post or organization that , among other attributes , carries on programs to perpetuate the memory of deceased veterans and members of the Armed Forces and to comfort their survivors , conducts programs for religious , charitable , scientific , literary , or educational purposes , sponsors or participates in activities of a patriotic nature , and provides social and recreational activities for their members . +| | ( d ) Section 215 . 1 of the Revenue and Taxation Code stipulates that all buildings , support and so much of the real property on which the buildings are situated as may be required for the convenient use and occupation of the buildings , used exclusively for charitable purposes , owned by a veterans \u2019 organization that has been chartered by the Congress of the United States , organized and operated for charitable purposes , when the same are used solely and exclusively for the purpose of the organization , if not conducted for profit and no part of the net earnings of which ensures to the benefit of any private individual or member thereof , are exempt from taxation . +| | ( e ) The Chief Counsel of the State Board of Equalization concluded , based on a 1979 appellate court decision , that only parts of American Legion halls are exempt from property taxation and that other parts , such as billiard rooms , card rooms , and similar areas , are not exempt . +| | ( f ) In a 1994 memorandum , the State Board of Equalization \u2019 s legal division further concluded that the areas normally considered eligible for exemptions are the office areas used to counsel veterans and the area used to store veterans \u2019 records , but that the meeting hall and bar found in most of the facilities are not considered used for charitable purposes . +| | ( g ) Tax - exempt status is intended to provide economic incentive and support to veterans \u2019 organizations to provide for the social welfare of the community of current and former military personnel . +| | ( h ) The State Board of Equalization \u2019 s constriction of the tax exemption has resulted in an onerous tax burden on California veteran service organizations posts or halls , hinders the posts \u2019 ability to provide facilities for veterans , and threatens the economic viability of many local organizations . +| | ( i ) The charitable activities of a veteran service organizations post or hall are much more than the counseling of veterans . The requirements listed for qualification for the federal tax exemption clearly dictate a need for more than just an office . +| | ( j ) Programs to perpetuate the memory of deceased veterans and members of the Armed Forces and to comfort their survivors require the use of facilities for funerals and receptions . +| | ( k ) Programs for religious , charitable , scientific , literary , or educational purposes require space for more than 50 attendees . +| | ( l ) Activities of a patriotic nature need facilities to accommodate hundreds of people . +| | ( m ) Social and recreational activities for members require precisely those areas considered \u201c not used for charitable purposes \u201d by the State Board of Equalization . +| | ( n ) The State Board of Equalization \u2019 s interpretation of the Revenue and Taxation Code reflects a lack of understanding of the purpose and programs of the veterans service organizations posts or halls and is detrimental to the good works performed in support of our veteran community . +| +| ( g ) Tax - exempt status is intended to provide economic incentive and support to veterans \u2019 organizations to provide for the social welfare of the community of current and former military personnel . +| | ( h ) The State Board of Equalization \u2019 s constriction of the tax exemption has resulted in an onerous tax burden on California veteran service organizations posts or halls , hinders the posts \u2019 ability to provide facilities for veterans , and threatens the economic viability of many local organizations . +| | ( i ) The charitable activities of a veteran service organizations post or hall are much more than the counseling of veterans . The requirements listed for qualification for the federal tax exemption clearly dictate a need for more than just an office . +| | ( j ) Programs to perpetuate the memory of deceased veterans and members of the Armed Forces and to comfort their survivors require the use of facilities for funerals and receptions . +| | ( k ) Programs for religious , charitable , scientific , literary , or educational purposes require space for more than 50 attendees . +| | ( l ) Activities of a patriotic nature need facilities to accommodate hundreds of people . +| | ( m ) Social and recreational activities for members require precisely those areas considered \u201c not used for charitable purposes \u201d by the State Board of Equalization . +| | ( n ) The State Board of Equalization \u2019 s interpretation of the Revenue and Taxation Code reflects a lack of understanding of the purpose and programs of the veterans service organizations posts or halls and is detrimental to the good works performed in support of our veteran community . +| | SECTION 1 . +| | SEC . 2 . +| | Section 215 . 1 of the Revenue and Taxation Code is amended to read : +| | 215 . 1 . +| | ( a ) All buildings , and so much of the real property on which the buildings are situated as may be required for the convenient use and occupation of the buildings , used exclusively for charitable purposes , owned by a veterans \u2019 organization that has been chartered by the Congress of the United States , organized and operated for charitable purposes , and exempt from federal income tax as an organization described in Section 501 ( c )( 19 ) of the Internal Revenue Code when the same are used solely and exclusively for the purpose of the organization , if not conducted for profit and no part of the net earnings of which inures to the benefit of any private individual or member thereof , shall be exempt from taxation . +| | ( b ) The exemption provided for in this section shall apply to the property of all organizations meeting the requirements of this section , subdivision ( b ) of Section 4 of Article XIII of the California Constitution , and paragraphs ( 1 ) to ( 4 ), inclusive , ( 6 ), and ( 7 ) of subdivision ( a ) of Section 214 . +| | ( c ) ( 1 ) The exemption specified by subdivision ( a ) shall not be denied to a property on the basis that the property is used for fraternal , lodge , or social club purposes . +| | ( 2 ) With regard to this subdivision , the Legislature finds and declares all of the following : +| | ( A ) The exempt activities of a veterans \u2019 organization as described in subdivision ( a ) qualitatively differ from the exempt activities of other nonprofit entities that use property for fraternal , lodge , or social club purposes in that the exempt purpose of the veterans \u2019 organization is to conduct programs to perpetuate the memory of deceased veterans and members of the Armed Forces and to comfort their survivors , to conduct programs for religious , charitable , scientific , literary , or educational purposes , to sponsor or participate in activities of a patriotic nature , and to provide social and recreational activities for their members . +| | ( B ) In light of this distinction , the use of real property by a veterans \u2019 organization as described in subdivision ( a ), for fraternal , lodge , or social club purposes is central to that organization \u2019 s exempt purposes and activities . +| | ( C ) In light of the factors set forth in subparagraphs ( A ) and ( B ), the use of real property by a veterans \u2019 organization as described in subdivision ( a ) for fraternal , lodge , or social club purposes , constitutes the exclusive use of that property for a charitable purpose within the meaning of subdivision ( b ) of Section 4 of Article XIII of the California Constitution . +| | ( d ) The exemption provided for in this section shall not apply to any portion of a property that consists of a bar where alcoholic beverages are served . The portion of the property ineligible for the veterans \u2019 organization exemption shall be that area used primarily to prepare and serve alcoholic beverages . +| | ( e ) An organization that files a claim for the exemption provided for in this section shall file with the assessor a valid organizational clearance certificate issued pursuant to Section 254 . 6 . +| | ( f ) This exemption shall be known as the \u201c veterans \u2019 organization exemption . \u201d +| | SEC . 2 . +| | SEC . 3 . +| | Notwithstanding Section 2229 of the Revenue and Taxation Code , no appropriation is made by this act and the state shall not reimburse any local agency for any property tax revenues lost by it pursuant to this act . +| | SEC . 3 . +| | SEC . 4 . +| | This act provides for a tax levy within the meaning of Article IV of the Constitution and shall go into immediate effect . | | ( 1 row ) This dataset has 3 fields, but summarization transformers only take a single input to produce their output. We can create a view that simply omits the title from the training data: omit_title.sql 1 2 CREATE OR REPLACE VIEW billsum_training_data AS SELECT \"text\" , summary FROM pgml . billsum ; Or, it might be interesting to concat the title to the text field to see how relevant it actually is to the bill. If the title of a bill is the first sentence, and doesn't appear in summary, it may indicate that it's a poorly chosen title for the bill: concat_title.sql 1 2 CREATE OR REPLACE VIEW billsum_training_data AS SELECT title || '\\n' || \"text\" AS \"text\" , summary FROM pgml . billsum ;","title":"Prepare the data"},{"location":"user_guides/transformers/fine_tuning/#tune-the-model_2","text":"Tuning has a nearly identical API to training, except you may pass the name of a model published on Hugging Face to start with, rather than training an algorithm from scratch. tune.sql 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 SELECT pgml . tune ( 'Legal Summarization' , task => 'summarization' , relation_name => 'billsum_training_data' , y_column_name => 'summary' , model_name => 'sshleifer/distilbart-xsum-12-1' , hyperparams => '{ \"learning_rate\": 2e-5, \"per_device_train_batch_size\": 2, \"per_device_eval_batch_size\": 2, \"num_train_epochs\": 1, \"weight_decay\": 0.01, \"max_input_length\": 1024, \"max_summary_length\": 128 }' , test_size => 0 . 2 , test_sampling => 'last' );","title":"Tune the model"},{"location":"user_guides/transformers/fine_tuning/#make-predictions_1","text":"SQL Result 1 SELECT pgml . predict ( 'IMDB Review Sentiment' , 'I love SQL' ) AS sentiment ; ```sql linenumes=\"1\" sentiment 1 (1 row) Time: 16.681 ms ``` The default for predict in a classification problem classifies the statement as one of the labels. In this case 0 is negative and 1 is positive. If you'd like to check the individual probabilities associated with each class you can use the predict_proba API SQL Result 1 SELECT pgml . predict_proba ( 'IMDB Review Sentiment' , 'I love SQL' ) AS sentiment ; ```sql linenumes=\"1\" sentiment [0.06266672909259796, 0.9373332858085632] (1 row) Time: 18.101 ms ``` This shows that there is a 6.26% chance for category 0 (negative sentiment), and a 93.73% chance it's category 1 (postive sentiment). See the task documentation for more examples, use cases, models and datasets.","title":"Make predictions"},{"location":"user_guides/transformers/pre_trained_models/","text":"Pre-Trained Models \u00b6 PostgresML integrates \ud83e\udd17 Hugging Face Transformers to bring state-of-the-art models into the data layer. There are tens of thousands of pre-trained models with pipelines to turn raw inputs into useful results. Many state of the art deep learning architectures have been published and made available for download. You will want to browse all the models available to find the perfect solution for your dataset and task . We'll demonstrate some of the tasks that are immediately available to users of your database upon installation: translation , sentiment analysis , summarization , question answering and text generation . Examples \u00b6 All of the tasks and models demonstrated here can be customized by passing additional arguments to the Pipeline initializer or call. You'll find additional links to documentation in the examples below. The Hugging Face Pipeline API is exposed in Postgres via: transformer.sql 1 2 3 4 5 pgml . transform ( task TEXT OR JSONB , -- task name or full pipeline initializer arguments call JSONB , -- additional call arguments alongside the inputs inputs TEXT [] OR BYTEA [] -- inputs for inference ) This is roughly equivalent to the following Python: transformer.py 1 2 3 4 import transformers def transform ( task , call , inputs ): return transformers . pipeline ( ** task )( inputs , ** call ) Most pipelines operate on TEXT[] inputs, but some require binary BYTEA[] data like audio classifiers. inputs can be SELECT ed from tables in the database, or they may be passed in directly with the query. The output of this call is a JSONB structure that is task specific. See the Postgres JSON reference for ways to process this output dynamically. Tip Models will be downloaded and stored locally on disk after the first call. They are also cached per connection to improve repeated calls in a single session. To free that memory, you'll need to close your connection. You may want to establish dedicated credentials and connection pools via pgcat or pgbouncer for larger models that have billions of parameters. You may also pass {\"cache\": false} in the JSON call args to prevent this behavior. Translation \u00b6 There are thousands of different pre-trained translation models between language pairs. They generally take a single input string in the \"from\" language, and translate it into the \"to\" language as a result of the call. PostgresML transformations provide a batch interface where you can pass an array of TEXT to process in a single call for efficiency. Not all language pairs have a default task name like this example of English to French. In those cases, you'll need to specify the desired model by name. You can see how to specify a model in the next example . Because this is a batch call with 2 inputs, we'll get 2 outputs in the JSONB. For a translation from English to French with the default pre-trained model: SQL Result 1 2 3 4 5 6 7 SELECT pgml . transform ( 'translation_en_to_fr' , inputs => ARRAY [ 'Welcome to the future!' , 'Where have you been all this time?' ] ) AS french ; 1 2 3 4 5 6 french ------------------------------------------------------------ [ { \"translation_text\" : \"Bienvenue \u00e0 l'avenir!\" } , { \"translation_text\" : \"O\u00f9 \u00eates-vous all\u00e9 tout ce temps?\" } ] See translation documentation for more options. Sentiment Analysis \u00b6 Sentiment analysis is one use of text-classification , but there are many others . This model returns both a label classification [\"POSITIVE\", \"NEUTRAL\", \"NEGATIVE\"] , as well as the score where 0.0 is perfectly negative, and 1.0 is perfectly positive. This example demonstrates specifying the model to be used rather than the task. The roberta-large-mnli model specifies the task of sentiment-analysis in it's default configuration, so we may omit it from the parameters. Because this is a batch call with 2 inputs, we'll get 2 outputs in the JSONB. SQL Result 1 2 3 4 5 6 7 SELECT pgml . transform ( '{\"model\": \"roberta-large-mnli\"}' :: JSONB , inputs => ARRAY [ 'I love how amazingly simple ML has become!' , 'I hate doing mundane and thankless tasks. \u2639\ufe0f' ] ) AS positivity ; 1 2 3 4 5 6 positivity ------------------------------------------------------ [ { \"label\" : \"NEUTRAL\" , \"score\" : 0 . 8143417835235596 } , { \"label\" : \"NEUTRAL\" , \"score\" : 0 . 7637073993682861 } ] See text classification documentation for more options and potential use cases beyond sentiment analysis. You'll notice the outputs are not great in this example. RoBERTa is a breakthrough model, that demonstrated just how important each particular hyperparameter is for the task and particular dataset regardless of how large your model is. We'll show how to fine tune models on your data in the next step. Summarization \u00b6 Sometimes we need all the nuanced detail, but sometimes it's nice to get to the point. Summarization can reduce a very long and complex document to a few sentences. One studied application is reducing legal bills passed by Congress into a plain english summary. Hollywood may also need some intelligence to reduce a full synopsis down to a pithy blurb for movies like Inception. SQL Result 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 SELECT pgml . transform ( 'summarization' , inputs => ARRAY [ ' Dominic Cobb is the foremost practitioner of the artistic science of extraction, inserting oneself into a subject''s dreams to obtain hidden information without the subject knowing, a concept taught to him by his professor father-in-law, Dr. Stephen Miles. Dom''s associates are Miles'' former students, who Dom requires as he has given up being the dream architect for reasons he won''t disclose. Dom''s primary associate, Arthur, believes it has something to do with Dom''s deceased wife, Mal, who often figures prominently and violently in those dreams, or Dom''s want to \"go home\" (get back to his own reality, which includes two young children). Dom''s work is generally in corporate espionage. As the subjects don''t want the information to get into the wrong hands, the clients have zero tolerance for failure. Dom is also a wanted man, as many of his past subjects have learned what Dom has done to them. One of those subjects, Mr. Saito, offers Dom a job he can''t refuse: to take the concept one step further into inception, namely planting thoughts into the subject''s dreams without them knowing. Inception can fundamentally alter that person as a being. Saito''s target is Robert Michael Fischer, the heir to an energy business empire, which has the potential to rule the world if continued on the current trajectory. Beyond the complex logistics of the dream architecture of the case and some unknowns concerning Fischer, the biggest obstacles in success for the team become worrying about one aspect of inception which Cobb fails to disclose to the other team members prior to the job, and Cobb''s newest associate Ariadne''s belief that Cobb''s own subconscious, especially as it relates to Mal, may be taking over what happens in the dreams. ' ] ) AS result ; 1 2 3 4 5 6 7 result -------------------------------------------------------------------------- [ { \"summary_text\" : \"Dominic Cobb is the foremost practitioner of the artistic science of extraction . his associates are former students, who Dom requires as he has given up being the dream architect . he is also a wanted man, as many of his past subjects have learned what Dom has done to them .\" } ] See summarization documentation for more options. Question Answering \u00b6 Question Answering extracts an answer from a given context. Recent progress has enabled models to also specify if the answer is present in the context at all. If you were trying to build a general question answering system, you could first turn the question into a keyword search against Wikipedia articles, and then use a model to retrieve the correct answer from the top hit. Another application would provide automated support from a knowledge base, based on the customers question. SQL Result 1 2 3 4 5 6 7 8 9 SELECT pgml . transform ( 'question-answering' , inputs => ARRAY [ '{ \"question\": \"Am I dreaming?\", \"context\": \"I got a good nights sleep last night and started a simple tutorial over my cup of morning coffee. The capabilities seem unreal, compared to what I came to expect from the simple SQL standard I studied so long ago. The answer is staring me in the face, and I feel the uncanny call from beyond the screen to check the results.\" }' ] ) AS answer ; 1 2 3 4 5 6 7 8 answer ----------------------------------------------------- { \"end\" : 36 , \"score\" : 0 . 20027603209018707 , \"start\" : 0 , \"answer\" : \"I got a good nights sleep last night\" } See question answering documentation for more options. Text Generation \u00b6 If you need to expand on some thoughts, you can have AI complete your sentences for you: SQL Result 1 2 3 4 5 SELECT pgml . transform ( 'text-generation' , '{\"num_return_sequences\": 2}' , ARRAY [ 'Three Rings for the Elven-kings under the sky, Seven for the Dwarf-lords in their halls of stone' ] ) AS result ; 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 result ----------------------------------------------------------------------------- [[ { \"generated_text\" : \"Three Rings for the Elven-kings under the sky, Seven for the Dwarf-lords in their halls of stone, and five for the Elves.\\nWhen, from all that's happening, he sees these things, he says to himself,\" } , { \"generated_text\" : \"Three Rings for the Elven-kings under the sky, Seven for the Dwarf-lords in their halls of stone, Eight for the Erogean-kings in their halls of stone -- \\\" and so forth ; \\ \" and \\\" of these \" } ]] More \u00b6 There are many different tasks and tens of thousands of state-of-the-art models available for you to explore. The possibilities are expanding every day. There can be amazing performance improvements in domain specific versions of these general tasks by fine tuning published models on your dataset. See the next section for fine tuning demonstrations.","title":"Pre-Trained Models"},{"location":"user_guides/transformers/pre_trained_models/#pre-trained-models","text":"PostgresML integrates \ud83e\udd17 Hugging Face Transformers to bring state-of-the-art models into the data layer. There are tens of thousands of pre-trained models with pipelines to turn raw inputs into useful results. Many state of the art deep learning architectures have been published and made available for download. You will want to browse all the models available to find the perfect solution for your dataset and task . We'll demonstrate some of the tasks that are immediately available to users of your database upon installation: translation , sentiment analysis , summarization , question answering and text generation .","title":"Pre-Trained Models"},{"location":"user_guides/transformers/pre_trained_models/#examples","text":"All of the tasks and models demonstrated here can be customized by passing additional arguments to the Pipeline initializer or call. You'll find additional links to documentation in the examples below. The Hugging Face Pipeline API is exposed in Postgres via: transformer.sql 1 2 3 4 5 pgml . transform ( task TEXT OR JSONB , -- task name or full pipeline initializer arguments call JSONB , -- additional call arguments alongside the inputs inputs TEXT [] OR BYTEA [] -- inputs for inference ) This is roughly equivalent to the following Python: transformer.py 1 2 3 4 import transformers def transform ( task , call , inputs ): return transformers . pipeline ( ** task )( inputs , ** call ) Most pipelines operate on TEXT[] inputs, but some require binary BYTEA[] data like audio classifiers. inputs can be SELECT ed from tables in the database, or they may be passed in directly with the query. The output of this call is a JSONB structure that is task specific. See the Postgres JSON reference for ways to process this output dynamically. Tip Models will be downloaded and stored locally on disk after the first call. They are also cached per connection to improve repeated calls in a single session. To free that memory, you'll need to close your connection. You may want to establish dedicated credentials and connection pools via pgcat or pgbouncer for larger models that have billions of parameters. You may also pass {\"cache\": false} in the JSON call args to prevent this behavior.","title":"Examples"},{"location":"user_guides/transformers/pre_trained_models/#translation","text":"There are thousands of different pre-trained translation models between language pairs. They generally take a single input string in the \"from\" language, and translate it into the \"to\" language as a result of the call. PostgresML transformations provide a batch interface where you can pass an array of TEXT to process in a single call for efficiency. Not all language pairs have a default task name like this example of English to French. In those cases, you'll need to specify the desired model by name. You can see how to specify a model in the next example . Because this is a batch call with 2 inputs, we'll get 2 outputs in the JSONB. For a translation from English to French with the default pre-trained model: SQL Result 1 2 3 4 5 6 7 SELECT pgml . transform ( 'translation_en_to_fr' , inputs => ARRAY [ 'Welcome to the future!' , 'Where have you been all this time?' ] ) AS french ; 1 2 3 4 5 6 french ------------------------------------------------------------ [ { \"translation_text\" : \"Bienvenue \u00e0 l'avenir!\" } , { \"translation_text\" : \"O\u00f9 \u00eates-vous all\u00e9 tout ce temps?\" } ] See translation documentation for more options.","title":"Translation"},{"location":"user_guides/transformers/pre_trained_models/#sentiment-analysis","text":"Sentiment analysis is one use of text-classification , but there are many others . This model returns both a label classification [\"POSITIVE\", \"NEUTRAL\", \"NEGATIVE\"] , as well as the score where 0.0 is perfectly negative, and 1.0 is perfectly positive. This example demonstrates specifying the model to be used rather than the task. The roberta-large-mnli model specifies the task of sentiment-analysis in it's default configuration, so we may omit it from the parameters. Because this is a batch call with 2 inputs, we'll get 2 outputs in the JSONB. SQL Result 1 2 3 4 5 6 7 SELECT pgml . transform ( '{\"model\": \"roberta-large-mnli\"}' :: JSONB , inputs => ARRAY [ 'I love how amazingly simple ML has become!' , 'I hate doing mundane and thankless tasks. \u2639\ufe0f' ] ) AS positivity ; 1 2 3 4 5 6 positivity ------------------------------------------------------ [ { \"label\" : \"NEUTRAL\" , \"score\" : 0 . 8143417835235596 } , { \"label\" : \"NEUTRAL\" , \"score\" : 0 . 7637073993682861 } ] See text classification documentation for more options and potential use cases beyond sentiment analysis. You'll notice the outputs are not great in this example. RoBERTa is a breakthrough model, that demonstrated just how important each particular hyperparameter is for the task and particular dataset regardless of how large your model is. We'll show how to fine tune models on your data in the next step.","title":"Sentiment Analysis"},{"location":"user_guides/transformers/pre_trained_models/#summarization","text":"Sometimes we need all the nuanced detail, but sometimes it's nice to get to the point. Summarization can reduce a very long and complex document to a few sentences. One studied application is reducing legal bills passed by Congress into a plain english summary. Hollywood may also need some intelligence to reduce a full synopsis down to a pithy blurb for movies like Inception. SQL Result 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 SELECT pgml . transform ( 'summarization' , inputs => ARRAY [ ' Dominic Cobb is the foremost practitioner of the artistic science of extraction, inserting oneself into a subject''s dreams to obtain hidden information without the subject knowing, a concept taught to him by his professor father-in-law, Dr. Stephen Miles. Dom''s associates are Miles'' former students, who Dom requires as he has given up being the dream architect for reasons he won''t disclose. Dom''s primary associate, Arthur, believes it has something to do with Dom''s deceased wife, Mal, who often figures prominently and violently in those dreams, or Dom''s want to \"go home\" (get back to his own reality, which includes two young children). Dom''s work is generally in corporate espionage. As the subjects don''t want the information to get into the wrong hands, the clients have zero tolerance for failure. Dom is also a wanted man, as many of his past subjects have learned what Dom has done to them. One of those subjects, Mr. Saito, offers Dom a job he can''t refuse: to take the concept one step further into inception, namely planting thoughts into the subject''s dreams without them knowing. Inception can fundamentally alter that person as a being. Saito''s target is Robert Michael Fischer, the heir to an energy business empire, which has the potential to rule the world if continued on the current trajectory. Beyond the complex logistics of the dream architecture of the case and some unknowns concerning Fischer, the biggest obstacles in success for the team become worrying about one aspect of inception which Cobb fails to disclose to the other team members prior to the job, and Cobb''s newest associate Ariadne''s belief that Cobb''s own subconscious, especially as it relates to Mal, may be taking over what happens in the dreams. ' ] ) AS result ; 1 2 3 4 5 6 7 result -------------------------------------------------------------------------- [ { \"summary_text\" : \"Dominic Cobb is the foremost practitioner of the artistic science of extraction . his associates are former students, who Dom requires as he has given up being the dream architect . he is also a wanted man, as many of his past subjects have learned what Dom has done to them .\" } ] See summarization documentation for more options.","title":"Summarization"},{"location":"user_guides/transformers/pre_trained_models/#question-answering","text":"Question Answering extracts an answer from a given context. Recent progress has enabled models to also specify if the answer is present in the context at all. If you were trying to build a general question answering system, you could first turn the question into a keyword search against Wikipedia articles, and then use a model to retrieve the correct answer from the top hit. Another application would provide automated support from a knowledge base, based on the customers question. SQL Result 1 2 3 4 5 6 7 8 9 SELECT pgml . transform ( 'question-answering' , inputs => ARRAY [ '{ \"question\": \"Am I dreaming?\", \"context\": \"I got a good nights sleep last night and started a simple tutorial over my cup of morning coffee. The capabilities seem unreal, compared to what I came to expect from the simple SQL standard I studied so long ago. The answer is staring me in the face, and I feel the uncanny call from beyond the screen to check the results.\" }' ] ) AS answer ; 1 2 3 4 5 6 7 8 answer ----------------------------------------------------- { \"end\" : 36 , \"score\" : 0 . 20027603209018707 , \"start\" : 0 , \"answer\" : \"I got a good nights sleep last night\" } See question answering documentation for more options.","title":"Question Answering"},{"location":"user_guides/transformers/pre_trained_models/#text-generation","text":"If you need to expand on some thoughts, you can have AI complete your sentences for you: SQL Result 1 2 3 4 5 SELECT pgml . transform ( 'text-generation' , '{\"num_return_sequences\": 2}' , ARRAY [ 'Three Rings for the Elven-kings under the sky, Seven for the Dwarf-lords in their halls of stone' ] ) AS result ; 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 result ----------------------------------------------------------------------------- [[ { \"generated_text\" : \"Three Rings for the Elven-kings under the sky, Seven for the Dwarf-lords in their halls of stone, and five for the Elves.\\nWhen, from all that's happening, he sees these things, he says to himself,\" } , { \"generated_text\" : \"Three Rings for the Elven-kings under the sky, Seven for the Dwarf-lords in their halls of stone, Eight for the Erogean-kings in their halls of stone -- \\\" and so forth ; \\ \" and \\\" of these \" } ]]","title":"Text Generation"},{"location":"user_guides/transformers/pre_trained_models/#more","text":"There are many different tasks and tens of thousands of state-of-the-art models available for you to explore. The possibilities are expanding every day. There can be amazing performance improvements in domain specific versions of these general tasks by fine tuning published models on your dataset. See the next section for fine tuning demonstrations.","title":"More"},{"location":"user_guides/transformers/setup/","text":"label img { position: relative; top: 0.3em; left: -0.1em; height: auto !important; width: 1.2em !important; } \ud83e\udd17 Transformers \u00b6 PostgresML integrates \ud83e\udd17 Hugging Face Transformers to bring state-of-the-art models into the data layer. There are tens of thousands of pre-trained models with pipelines to turn raw inputs into useful results. Many state of the art deep learning architectures have been published and made available for download. You will want to browse all the models available to find the perfect solution for your dataset and task . Setup \u00b6 Install the machine learning depedencies on the database for the transformers you would like to use: PyTorch Tensorflow Flax See the Pytorch docs for more information. $ sudo pip3 install torch See the Tensorflow docs for more information. $ sudo pip3 install tensorflow See the Flax docs for more information. $ sudo pip3 install flax Models will be downloaded and cached on the database for repeated usage. View the Transformers installation docs for cache management details and offline deployments. You may also want to install GPU support when working with larger models. Standard Datasets \u00b6 Many datasets have been published to stimulate research and benchmark architectures, but also to help demonstrate API usage in the tutorials. The Datasets package provides a way to load published datasets into Postgres: $ sudo pip3 install datasets Audio Processing \u00b6 Torch Audio is required for many models that process audio data. You can install the additional dependencies with: $ sudo pip3 install torchaudio","title":"Setup"},{"location":"user_guides/transformers/setup/#transformers","text":"PostgresML integrates \ud83e\udd17 Hugging Face Transformers to bring state-of-the-art models into the data layer. There are tens of thousands of pre-trained models with pipelines to turn raw inputs into useful results. Many state of the art deep learning architectures have been published and made available for download. You will want to browse all the models available to find the perfect solution for your dataset and task .","title":"\ud83e\udd17 Transformers"},{"location":"user_guides/transformers/setup/#setup","text":"Install the machine learning depedencies on the database for the transformers you would like to use: PyTorch Tensorflow Flax See the Pytorch docs for more information. $ sudo pip3 install torch See the Tensorflow docs for more information. $ sudo pip3 install tensorflow See the Flax docs for more information. $ sudo pip3 install flax Models will be downloaded and cached on the database for repeated usage. View the Transformers installation docs for cache management details and offline deployments. You may also want to install GPU support when working with larger models.","title":"Setup"},{"location":"user_guides/transformers/setup/#standard-datasets","text":"Many datasets have been published to stimulate research and benchmark architectures, but also to help demonstrate API usage in the tutorials. The Datasets package provides a way to load published datasets into Postgres: $ sudo pip3 install datasets","title":"Standard Datasets"},{"location":"user_guides/transformers/setup/#audio-processing","text":"Torch Audio is required for many models that process audio data. You can install the additional dependencies with: $ sudo pip3 install torchaudio","title":"Audio Processing"},{"location":"user_guides/vector_operations/overview/","text":"Vector Operations \u00b6 PostgresML adds native vector operations that can be used in SQL queries. Vector operations are particularly useful for dealing with embeddings that have been generated from other machine learning algorithms and can provide functions like nearest neighbor calculations using the distance functions. Emeddings can be a relatively efficient mechanism to leverage the power deep learning, without the runtime inference costs. These functions are relatively fast and the more expensive distance functions can compute ~100k per second for a memory resident dataset on modern hardware. The PostgreSQL planner will also automatically parallelize evalualtion on larger datasets, as configured to take advantage of multiple CPU cores when available. Nearest neighbor example \u00b6 If we had precalculated the embeddings for a set of user and product data, we could find the 100 best products for a user with a similarity search. 1 2 3 4 5 6 7 8 SELECT products . id , pgml . cosine_similarity ( users . embedding , products . embedding ) AS distance FROM users JOIN products WHERE users . id = 123 ORDER BY distance ASC LIMIT 100 ; Elementwise arithmetic w/ constants \u00b6 Addition \u00b6 1 pgml . add ( a REAL [], b REAL ) -> REAL [] Subtraction \u00b6 1 pgml . subtract ( minuend REAL [], subtrahend REAL ) -> REAL [] Multiplication \u00b6 1 pgml . multiply ( multiplicand REAL [], multiplier REAL ) -> REAL [] Division \u00b6 1 pgml . divide ( dividend REAL [], divisor REAL ) -> REAL [] Pairwise arithmetic w/ vectors \u00b6 Addition \u00b6 1 pgml . add ( a REAL [], b REAL []) -> REAL [] Subtraction \u00b6 1 pgml . subtract ( minuend REAL [], subtrahend REAL []) -> REAL [] Multiplication \u00b6 1 pgml . multiply ( multiplicand REAL [], multiplier REAL []) -> REAL [] Division \u00b6 1 pgml . divide ( dividend REAL [], divisor REAL []) -> REAL [] Norms \u00b6 Dimensions not at origin \u00b6 1 pgml . norm_l0 ( vector REAL []) -> REAL Manhattan distance from origin \u00b6 1 pgml . norm_l1 ( vector REAL []) -> REAL Euclidean distance from origin \u00b6 1 pgml . norm_l2 ( vector REAL []) -> REAL Absolute value of largest element \u00b6 1 pgml . norm_max ( vector REAL []) -> REAL Normalization \u00b6 Unit Vector \u00b6 1 pgml . normalize_l1 ( vector REAL []) -> REAL [] Squared Unit Vector \u00b6 1 pgml . normalize_l2 ( vector REAL []) -> REAL [] -1:1 values \u00b6 1 pgml . normalize_max ( vector REAL []) -> REAL [] Distances \u00b6 Manhattan \u00b6 1 pgml . distance_l1 ( a REAL [], b REAL []) -> REAL Euclidean \u00b6 1 pgml . distance_l2 ( a REAL [], b REAL []) -> REAL Projection \u00b6 1 pgml . dot_product ( a REAL [], b REAL []) -> REAL Direction \u00b6 1 pgml . cosine_similarity ( a REAL [], b REAL []) -> REAL","title":"Vector Operations"},{"location":"user_guides/vector_operations/overview/#vector-operations","text":"PostgresML adds native vector operations that can be used in SQL queries. Vector operations are particularly useful for dealing with embeddings that have been generated from other machine learning algorithms and can provide functions like nearest neighbor calculations using the distance functions. Emeddings can be a relatively efficient mechanism to leverage the power deep learning, without the runtime inference costs. These functions are relatively fast and the more expensive distance functions can compute ~100k per second for a memory resident dataset on modern hardware. The PostgreSQL planner will also automatically parallelize evalualtion on larger datasets, as configured to take advantage of multiple CPU cores when available.","title":"Vector Operations"},{"location":"user_guides/vector_operations/overview/#nearest-neighbor-example","text":"If we had precalculated the embeddings for a set of user and product data, we could find the 100 best products for a user with a similarity search. 1 2 3 4 5 6 7 8 SELECT products . id , pgml . cosine_similarity ( users . embedding , products . embedding ) AS distance FROM users JOIN products WHERE users . id = 123 ORDER BY distance ASC LIMIT 100 ;","title":"Nearest neighbor example"},{"location":"user_guides/vector_operations/overview/#elementwise-arithmetic-w-constants","text":"","title":"Elementwise arithmetic w/ constants"},{"location":"user_guides/vector_operations/overview/#addition","text":"1 pgml . add ( a REAL [], b REAL ) -> REAL []","title":"Addition"},{"location":"user_guides/vector_operations/overview/#subtraction","text":"1 pgml . subtract ( minuend REAL [], subtrahend REAL ) -> REAL []","title":"Subtraction"},{"location":"user_guides/vector_operations/overview/#multiplication","text":"1 pgml . multiply ( multiplicand REAL [], multiplier REAL ) -> REAL []","title":"Multiplication"},{"location":"user_guides/vector_operations/overview/#division","text":"1 pgml . divide ( dividend REAL [], divisor REAL ) -> REAL []","title":"Division"},{"location":"user_guides/vector_operations/overview/#pairwise-arithmetic-w-vectors","text":"","title":"Pairwise arithmetic w/ vectors"},{"location":"user_guides/vector_operations/overview/#addition_1","text":"1 pgml . add ( a REAL [], b REAL []) -> REAL []","title":"Addition"},{"location":"user_guides/vector_operations/overview/#subtraction_1","text":"1 pgml . subtract ( minuend REAL [], subtrahend REAL []) -> REAL []","title":"Subtraction"},{"location":"user_guides/vector_operations/overview/#multiplication_1","text":"1 pgml . multiply ( multiplicand REAL [], multiplier REAL []) -> REAL []","title":"Multiplication"},{"location":"user_guides/vector_operations/overview/#division_1","text":"1 pgml . divide ( dividend REAL [], divisor REAL []) -> REAL []","title":"Division"},{"location":"user_guides/vector_operations/overview/#norms","text":"","title":"Norms"},{"location":"user_guides/vector_operations/overview/#dimensions-not-at-origin","text":"1 pgml . norm_l0 ( vector REAL []) -> REAL","title":"Dimensions not at origin"},{"location":"user_guides/vector_operations/overview/#manhattan-distance-from-origin","text":"1 pgml . norm_l1 ( vector REAL []) -> REAL","title":"Manhattan distance from origin"},{"location":"user_guides/vector_operations/overview/#euclidean-distance-from-origin","text":"1 pgml . norm_l2 ( vector REAL []) -> REAL","title":"Euclidean distance from origin"},{"location":"user_guides/vector_operations/overview/#absolute-value-of-largest-element","text":"1 pgml . norm_max ( vector REAL []) -> REAL","title":"Absolute value of largest element"},{"location":"user_guides/vector_operations/overview/#normalization","text":"","title":"Normalization"},{"location":"user_guides/vector_operations/overview/#unit-vector","text":"1 pgml . normalize_l1 ( vector REAL []) -> REAL []","title":"Unit Vector"},{"location":"user_guides/vector_operations/overview/#squared-unit-vector","text":"1 pgml . normalize_l2 ( vector REAL []) -> REAL []","title":"Squared Unit Vector"},{"location":"user_guides/vector_operations/overview/#-11-values","text":"1 pgml . normalize_max ( vector REAL []) -> REAL []","title":"-1:1 values"},{"location":"user_guides/vector_operations/overview/#distances","text":"","title":"Distances"},{"location":"user_guides/vector_operations/overview/#manhattan","text":"1 pgml . distance_l1 ( a REAL [], b REAL []) -> REAL","title":"Manhattan"},{"location":"user_guides/vector_operations/overview/#euclidean","text":"1 pgml . distance_l2 ( a REAL [], b REAL []) -> REAL","title":"Euclidean"},{"location":"user_guides/vector_operations/overview/#projection","text":"1 pgml . dot_product ( a REAL [], b REAL []) -> REAL","title":"Projection"},{"location":"user_guides/vector_operations/overview/#direction","text":"1 pgml . cosine_similarity ( a REAL [], b REAL []) -> REAL","title":"Direction"}]}